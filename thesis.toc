\select@language {english}
\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.1}
\contentsline {section}{\numberline {1.1}Problem Definition}{3}{section.1.1}
\contentsline {section}{\numberline {1.2}Objectives}{3}{section.1.2}
\contentsline {section}{\numberline {1.3}Thesis Structure}{4}{section.1.3}
\contentsline {chapter}{\numberline {2}Theoretical Foundations}{5}{chapter.2}
\contentsline {section}{\numberline {2.1}Literature Overview}{5}{section.2.1}
\contentsline {subsection}{\numberline {2.1.1}Queueing}{5}{subsection.2.1.1}
\contentsline {subsection}{\numberline {2.1.2}Workflow}{5}{subsection.2.1.2}
\contentsline {subsection}{\numberline {2.1.3}Reinforcement Learning}{7}{subsection.2.1.3}
\contentsline {subsection}{\numberline {2.1.4}Optimization}{8}{subsection.2.1.4}
\contentsline {subsection}{\numberline {2.1.5}Simulation}{8}{subsection.2.1.5}
\contentsline {section}{\numberline {2.2}Research Deficit}{9}{section.2.2}
\contentsline {chapter}{\numberline {3}Methodology}{11}{chapter.3}
\contentsline {section}{\numberline {3.1}Analysis Structure}{11}{section.3.1}
\contentsline {subsection}{\numberline {3.1.1}Tools}{11}{subsection.3.1.1}
\contentsline {subsection}{\numberline {3.1.2}Discrete event simulation using SimPy}{11}{subsection.3.1.2}
\contentsline {subsection}{\numberline {3.1.3}Analysis Environment}{12}{subsection.3.1.3}
\contentsline {subsubsection}{Start event}{12}{section*.8}
\contentsline {subsubsection}{User task}{13}{section*.9}
\contentsline {subsubsection}{Policy}{14}{section*.10}
\contentsline {section}{\numberline {3.2}Optimization Policies}{15}{section.3.2}
\contentsline {section}{\numberline {3.3}Reinforcement Learning Theory}{21}{section.3.3}
\contentsline {subsection}{\numberline {3.3.1}Reinforcement Learning Definition}{21}{subsection.3.3.1}
\contentsline {subsection}{\numberline {3.3.2}Finite Markov Decision Processes}{21}{subsection.3.3.2}
\contentsline {subsection}{\numberline {3.3.3}Dynamic Programming}{22}{subsection.3.3.3}
\contentsline {subsection}{\numberline {3.3.4}Monte Carlo Methods}{22}{subsection.3.3.4}
\contentsline {subsection}{\numberline {3.3.5}Temporal-Difference Learning}{23}{subsection.3.3.5}
\contentsline {subsection}{\numberline {3.3.6}On-policy Prediction with Approximation}{23}{subsection.3.3.6}
\contentsline {subsection}{\numberline {3.3.7}On-policy Control with Approximation}{24}{subsection.3.3.7}
\contentsline {subsection}{\numberline {3.3.8}Off-policy Methods with Approximation}{24}{subsection.3.3.8}
\contentsline {subsection}{\numberline {3.3.9}Policy Gradient Methods}{25}{subsection.3.3.9}
\contentsline {subsubsection}{Policy Gradient with Baseline}{26}{section*.11}
\contentsline {subsubsection}{Actor-Critic Policy Gradient}{26}{section*.12}
\contentsline {section}{\numberline {3.4}Reinforcement Learning Policies}{26}{section.3.4}
\contentsline {subsection}{\numberline {3.4.1}Prediction and Control Methods}{27}{subsection.3.4.1}
\contentsline {subsubsection}{Value Function Approximation}{27}{section*.13}
\contentsline {subsubsection}{Policy Gradient}{29}{section*.14}
\contentsline {section}{\numberline {3.5}Hypothesis}{29}{section.3.5}
\contentsline {section}{\numberline {3.6}Data}{29}{section.3.6}
\contentsline {chapter}{\numberline {4}Empirical Analysis}{31}{chapter.4}
\contentsline {section}{\numberline {4.1}Results}{31}{section.4.1}
\contentsline {section}{\numberline {4.2}Discussion}{31}{section.4.2}
\contentsline {section}{\numberline {4.3}Research Contribution}{31}{section.4.3}
\contentsline {chapter}{\numberline {5}Conclusion}{33}{chapter.5}
\contentsline {section}{\numberline {5.1}Summary}{33}{section.5.1}
\contentsline {section}{\numberline {5.2}Resulting Conclusions}{33}{section.5.2}
\contentsline {section}{\numberline {5.3}Outlook}{33}{section.5.3}
\contentsline {chapter}{\numberline {A}First}{35}{appendix.A}
\contentsline {chapter}{\numberline {B}Second}{37}{appendix.B}
