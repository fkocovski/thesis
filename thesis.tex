\documentclass{seal_thesis}

\usepackage{url}
\usepackage[inline]{enumitem}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage[colorlinks=true]{hyperref}
% used to fix structure
\usepackage{bookmark}
\usepackage{amsfonts}
\usepackage[acronym]{glossaries}

\makeglossaries

\loadglsentries{glossaries}

% see http://tex.stackexchange.com/questions/183149/cant-silence-a-pdftex-pdf-inclusion-multiple-pdfs-with-page-group-error for more information
\pdfsuppresswarningpagegroup=1

\thesisType{Master Thesis}
\date{\today}
\title{BPM}
\subtitle{Discrete Event Simulation for Optimal Role Resolution in Workflow Processes}
\author{Filip Ko\v{c}ovski}
\home{Lugano} % Geburtsort
\country{Switzerland}
\legi{10-932-994}
\prof{Prof. Dr. Daning Hu}
\assistent{Dr. Markus Uhr}
\email{filip.kocovski@uzh.ch}
\begindate{November 15, 2016}
\enddate{May 15, 2017}

\begin{document}
\maketitle

\frontmatter

\begin{acknowledgements}

\begin{itemize}
	\item Prof. Dr. Daning Hu $\rightarrow$ for letting me do this thesis at his chair
	\item Dr. Markus Uhr $\rightarrow$ for basically everything...and more
	\item Dr. Robert H\"ohener $\rightarrow$ for giving me the opportunity to do the thesis at work plus consulting me during the initial phase
	\item Dr. Thomas H\"ohener $\rightarrow$ for giving me the opportunity to do the thesis at work
	\item Dr. Andreas Horni $\rightarrow$ for proof reading the thesis and technical inputs for \LaTeX 
	\item Mom and Dad $\rightarrow$ for supporting me and my decisions in life
	\item Martina $\rightarrow$ for proofreading
	\item Rest of immopac ag staff $\rightarrow$ for continuous support and interest in the thesis
\end{itemize}

\end{acknowledgements}

\begin{abstract}

\end{abstract}

\begin{zusammenfassung}

\end{zusammenfassung}

\tableofcontents

\clearpage

\listoffigures

\clearpage

\listoftables

\mainmatter

\chapter{Introduction}
\label{ch:intro}

\section{Problem Definition}

Workflows are IT solutions that can help increase efficiency and get tasks done better and faster. However a key element of each workflow process still remains the human aspect. This human aspect can take many facets, such as humans analyzing a process, humans designing a process and humans executing the latter. This thesis focuses on the latter \ie where human agents interact with the workflow process in order to work on tasks. A business process that has been efficiently analyzed and subsequently optimally implemented still cannot ensure optimal execution, or no optimal execution can be achieved while a human intervention for task execution is present. It is here that optimal role resolution comes in play: optimally choosing and assigning a specific task inside the workflow process to the best possible actor is a non trivial task that has to be solved in order to close the ``optimization'' circle that workflow engines advertise.

This field is relevant since an optimal role resolution can bring optimization from many sides:
\begin{enumerate*}
	\item cost savings,
	\item fairness in workload assignment
	\item optimal resources usage.
\end{enumerate*}

Currently many different workflow engines exist, ranging from complete fully functional suites and down to extensible frameworks that allow the implementer to adapt it to its own needs. However all these solutions lack optimality in the task assignment sector.

\section{Objectives}
\label{sec:objectives}

The objectives of these thesis build upon the work of Zeng and Zhao \cite{Zeng2005}, in which they depicted preliminary policies for optimal role resolution, and extends these capabilities from a threefold perspective:
\begin{enumerate*}
	\item further develops the mathematical premises and extends the capabilities of the batching policies proposed by Zeng and Zhao
	\item explores the capabilities offered by \gls{rl} as addition and improvement for even precises, faster and better task assignment
	\item deployment of the aforementioned optimization techniques in an operative environment of a real estate company using a workflow engine.
\end{enumerate*}

Formally, this thesis tries to answer the following research questions:

\begin{enumerate}
	\item Are there better optimization techniques for optimal role resolution techniques inside workflow processes?
	\item Is the deployment of optimization policies in a working environment for a workflow engine a critical success factor?
	\item How is optimization in the field of task assignment perceived by the workflow users (actors)?
\end{enumerate}

\section{Thesis Structure}

This thesis is subdivided in five main chapters:

\begin{itemize}
	\item \chpref{ch:intro} gives an overview of why the chosen topic is relevant, what is the current context of the work and how this work fits in. It moreover articulates the central research questions that permeate this thesis and gives an overview of this essay
	\item \chpref{ch:foundations} gives an overview of the most important conceptual definitions and the state of the art literature review in the touched thematic topics of this work. Conclusively this chapter critically reflects upon the existing literature and exposes the deficits that this thesis aims filling
	\item \chpref{ch:methodology} gives an overview of the approach used for the research \eg the analysis environment and the used tools, states the hypothesis that wants to be proved and eventually describes statistically and qualitatively the data sets upon which the methodology is applied
	\item \chpref{ch:empirical_analysis} builds upon \chpref{ch:methodology} and makes its way into the hypothesis test field and the respective analysis results. Furthermore looks introspectively on the data correlation and gives an interpretation of the latter. Eventually in this section a statement about the contribution that the results bring into this field is given
	\item \chpref{ch:conclusion} is the culminating chapter in which a summary of the key findings of the thesis are outlined, the research questions posed in \secref{sec:objectives} are answered by looking at the actual usability, limitations and to whom the results are most applicable. Finally outlooks about the future trends and how the empirical results of this thesis can be extended by prospective researchers.
\end{itemize}

\chapter{Theoretical Foundations}
\label{ch:foundations}

\section{Literature Overview}
\label{sec:literature_overview}

This section servers as an overview of the state of the art literature that exists and has been used as a foundation basis for this work. \secref{sec:literature_overview} is divided in different thematic subsections.

\subsection{Queueing}

Queuing is a topic that talks about how people or more general agents are to be server while waiting.

Starting with one of the most notable contributions to this field done by Kendall in 1953 and his work on the Markov chains in queuing theory, where he formally defines different types of queues \cite{Kendall1953}.

In 2016, Adan describes the necessary basic concepts for queuing theory and an important topic here is the statistical foundation outlined in his work about different modeling techniques for randomized generation rates, such as the Erlang's distributions \cite{Adan2016}.

Pinedo outlines in his work in 2008 the most prominent key metrics that can be used in order to assert and measure queues performance \cite{Pinedo2008}.

Sun and Zhao in their work cover the aspect of formal analysis for workflow models and they claim that it should help ``...alleviating the intellectual challenge faced by business analysts when creating workflow models'' \cite{Sun2013}.

\subsection{Workflow}
\label{subsec:workflow}

A good starting point in the workflow thematic is Macintosh's work in which he gives an overview of the five levels of process maturity \cite{Macintosh1993}:
\begin{enumerate*}
	\item Initial, the process has to be set up
	\item Repeatable, the process has to be repeatable
	\item Defined, documentation standardization of processes
	\item Managed, measurement and control of processes
	\item Optimizing, continuous process improvement
\end{enumerate*}

Even though Georgakopoulos' work dates back to 1995, he still gives a comprehensive business oriented overview of the different workflow technologies present on the market \cite{Georgakopoulos1995}.

On this note, Giaglis lays out four different process perspectives:
\begin{enumerate*}
	\item Functional
	\item Behavioral
	\item Organizational
	\item Informational
\end{enumerate*}

His framework focuses on three dimensions:
\begin{enumerate*}
	\item Breadth, where modeling goals are typically addressed by technique
	\item Depth, where modeling perspectives are covered
	\item Fit, where typical project to which techniques can be fit
\end{enumerate*}

The presented framework is used to combine the three different dimensions in order to assert a possible best fit of a specific modeling technique based on which approach to be used under the constraints of a modeling perspective to cover \cite{Giaglis2001}.

Mentzas focuses on a qualitative level on how workflow technologies can facilitate implementation of business processes by focusing on the pros and cons of adopting alternative workflow modeling techniques \cite{Mentzas2001}. Moreover he formally defines what a workflow management system is and subdivides it in three main categories:
\begin{enumerate*}
	\item Process modeling
	\item Process re-engineering
	\item Workflow implementation and automation
\end{enumerate*}

Each level of maturity as defined by Macintosh requires a different model, such as the first three levels might require more descriptive models whereas levels four and five require decision support keen models in order to monitor and control processes \cite{Mentzas2001}.

Aguilar describes the main modeling techniques existing with workflow being one of them \cite{Aguilar-Saven2004}.

The key core topics on which this thesis lays its foundations upon is the work done by Zeng in 2005. Effective role resolution \ie the mechanism of assigning tasks to individual workers at runtime according to the role qualification defined in the workflow model \cite{Zeng2005}, is the core aspect that is being extended during this thesis work.

Zeng differentiates between staffing decisions and role resolution, with the former being the assignment one or more role to each user and the latter being the assignment of a specific task to an appropriate worker at runtime \cite{Zeng2005}. Staffing decisions are usually made off-line and periodically, thus being more of a strategic nature \cite{Zeng2005}. If role resolution were to be made on-line it could translate to a major operational level decision \ie the differentiation between strategic vs. operational playing role \cite{Zeng2005}.

He moreover defines three roles a workflow can fulfill:
\begin{enumerate*}
	\item System built-in policies
	\item User customizable policies
	\item Rule based policies
\end{enumerate*}

Considering capacities of resources restrictions under the assignment problem is an NP-hard computational problem and in his work Zeng focuses on how to solve the assignment problem and scheduling decisions with consideration of worker's preferences \cite{Zeng2005}. For this purpose he defines five workflow resolution policies:
\begin{enumerate*}
	\item \gls{llqp}
	\item \gls{sq}
	\item K-Batch
	\item K-Batch-1
	\item 1-Batch-1
\end{enumerate*}

For all batching policies a simplified version of \gls{dmf} has to be solved \cite{Zeng2005}.

Zeng's key findings are outlined as follows:
\begin{enumerate*}
	\item Batching policies to be used when system load is medium to high
	\item Processing time variation has major impact on system performance \ie higher variation favors optimization based policies
	\item Average workload and workload variation can be significantly reduced by online optimization
	\item 1-Batch-1 online optimization policy yields best results in operational conditions
\end{enumerate*}

Interestingly enough, workflow implementation in real world cases is not always only coupled with directly measurable effects, sometimes even unexpected results happen. What is called the ``workflow paradox'' according to Reijers is the fact that the very fact of companies accepting requests for workflow introduction might actually be the most promising way that leads to potentially better and more suitable alternatives \cite{Reijers2005}.

Specifically speaking on the data flow inside workflow processes, one has to consider possible anomalies that might happens. This has been extensively studies by Sun \etal where they formally define data flow methodologies for detecting such anomalies \cite{Sun2006}. Their framework is divided in two components:
\begin{enumerate*}
	\item Data flow specification
	\item Data flow analysis
\end{enumerate*}

Yet again we stumble upon mentioning that simulation for workflow management systems is usually inefficient and inaccurate \cite{Sun2006}. They moreover discuss aspects that data requirements have been analyzed but the required methodologies on discovering data flow errors have not been extensively researched \cite{Sun2006}.

A more recent taxonomy of different BPM application is given by a collaboration between SAP and accenture in 2009 \cite{EvolvedTechnologist2009}.

In the realm of workflow processes and engines BPMN's notation permeates the field and the work of Silver summarizes these foundations very well \cite{Silver2011}.

An analysis of the critical success factors (CSF) for BPM is required in order to assert a product validity and this has been done by Trkman where he defines CSF from three perspectives \cite{Trkman2010}:
\begin{enumerate*}
	\item Contingency theory
	\item Dynamic capabilities
	\item Task-technology fit theory
\end{enumerate*}


Change management in workflow is yet another interesting aspect that should be considered and this has been broadly studied by Wang where he developed an analytical framework for workflow change management through formal modeling of workflow constraints \cite{Wang2011}.

In companies different types of workflow models can exist and Fan focuses on two of these, namely:
\begin{enumerate*}
	\item Conceptual
	\item Logical
\end{enumerate*}

Conceptual models serve as documentation for generic process requirements whereas logical models are used as definitions for technology oriented requirements \cite{Fan2012}. One difficult aspect is the transition from the former to the latter and Fan proposes a formal approach to efficiently support such transitions \cite{Fan2012}.

\subsection{\glsentrylong{rl}}

\gls{rl} is a branch of machine learning that promises to overcome the drawbacks posed by the latter by not requiring a training set for efficient machine decisions.

One of the first \gls{mc} based \gls{pg} methods is the algorithm proposed by Williams called REINFORCE \cite{Williams1992}.

\gls{pg} methods with \gls{vfa} and their convergence is of vital importance and this can be achieved by representing the policy by an own function approximation which is independent of the value function and it is updated according to gradient of the expected rewards with respect to the afore mentioned policy \cite{Sutton1999}.

Discretization of the state action space is not always feasible and different techniques have to be used for tractability. Smith proposes such an approach which he calls ``self-organizing map'' \cite{Smith2002}.

As Markov Decision Processes grow in size, so does the required computational memory to solve possible discrete lookup tables modeling the state-actions spaces that characterizes them. Notable examples that show how large some of the most common problems can be:
\begin{enumerate*}
	\item the game of backgammon has a total of $10^{20}$ states
	\item the traditional Chinese abstract board game Go has an estimated total of $10^{170}$ states
	\item flying a helicopter or having a robot move in space all require a continuous state space.
\end{enumerate*}

This huge state space requirement is a clear limitation to lookup tables. Even if memory would not be a constraint, the actual learning from such tables would be infeasible. In order to pragmatically learn by reinforcement on such huge problems, \gls{vfa} in the domain of \gls{rl} proves to be a viable solution. For different types of \gls{rl} approaches \ie \gls{mc} or \gls{td} methods exist different types of \gls{vfa}, ranging from simple linear combinations of features for \gls{mc} to \glspl{ann} for \gls{td} learning. All these different methodologies are outlined in the tutorial by Geramifard \cite{Geramifard2013}.

When working with on-line algorithms such as \gls{td}(0) it is important to choose correct parameters for an effective learning process, otherwise the learning algorithm put in place might never converge towards an optimal solution. This aspect is being discussed by Korda in which he depicts different non-asymptotic bounds for the \gls{td} learning algorithms \cite{Korda2014}.

There are two main fields in \gls{rl}, one is using \gls{vfa} for either the state value function or for using control mechanisms with the state action value function, while the other one is using \gls{pg} methods for policy optimization. The latter offers different methods such as the naive finite difference methods, \gls{mc} based \gls{pg} methods and finally actor critic \gls{pg} methods \cite{Silver2014}.

Notable works in the field of \gls{rl} and its application include Google DeepMind work on novel algorithms for tackling fields previously barely scratched, as mentioned by Mnih \etal and Silver \etal \cite{Mnih2015,Silver2016}.

Sutton started working on the \gls{rl} topic in the early nineties and is now planning his third edition of his famous book on machine learning, which is due in 2017 \cite{Sutton2017}. In our case \gls{rl} is used in order for the policies to be able to alone get better by continuously analyzing their own decision models and optimize upon them.

\subsection{Optimization}

For all batching policies implemented in this work, a mixed integer optimization was solved in order to optimally assign jobs to users in the workflow processes. The generalized assignment problem is a very well known problem in combinatorial mathematics. Cattrysse gives an overview of different algorithms for solving the generalized assignment problem \cite{Cattrysse1992}. Heuristics are also a viable solution for solving such adaptation of the generalized assignment problem, as Racer states \cite{Racer1994}. Moreover a global perspective of optimization from a mathematical perspective is given in Boyd's work on convex optimization \cite{Boyd2004}.

Last but not least, according to the AIMMS guidelines, there are different linear programming tricks that can be used to shape such problems in solvable outlines \cite{Bisschop2016}. In this thesis, a specific linear programming trick, called either-or constraints, was used by adding so called auxiliary variables to the evaluation method presented in order to efficiently solve an otherwise non solvable equation \cite[p. 77]{Bisschop2016}.

\subsection{Simulation}

Simulating queues can prove to be arduous. The main differentiation needed here is that between continuous and step functions: the former is the result when the events being simulated yield values that if plotted against the simulation time give a continuous function. On the other hand, if we simulate events that yield discrete values, such as inventory changes in a storage facility and plot the results against the simulation time we would get so called step functions \cite{Matloff2008}.

According to Matloff, there exist different world views for discrete event programing, as he calls them paradigms \cite{Matloff2008}:
\begin{enumerate*}
	\item Activity oriented
	\item Event oriented
	\item Process oriented
\end{enumerate*}

Activity oriented can be summarized as simulation events where time is being subdivided in tiny intervals at which the program checks the status for all simulated entities. Since petite subdivisions of time are possible in such types of simulations, it is clear that the program might prove tedious, since most of the time there won't be any change in state for the simulated entities \cite{Matloff2008}. Event oriented circumnavigate this issue by advancing the simulation time directly to the next event to be simulated. By filling these gaps, a dramatical increase in computation can be observed \cite{Matloff2008}. Last but not least, the process oriented simulation models each simulation activity as a process or thread. Management of threads has steadily decreased in todays computation since many different packages for governing such tasks.

On another note, Bahouth focuses in work on algorithmic analysis of discrete event simulation supplemented with focus on factors such as compiler efficiency, code interpretation and caching memory issues \cite{Bahouth2007}. According to his findings, a significant speedup can be achieved if one addresses the afore mentioned facets.

\section{Research Deficit}

\chapter{Methodology}
\label{ch:methodology}

\section{Analysis Structure}
\subsection{Tools}
Different tools were used in the analysis environment in order to efficiently simulate and analyze the work of this thesis.

The whole architecture is subdivided as follows:
\begin{enumerate*}
	\item The simulation environment is based on \texttt{Python 3.5.2}\foot{https://www.python.org}{06.01.2017} using the \texttt{Anaconda}\foot{https://www.continuum.io/anaconda-overview}{03.04.2017} platform and as a discrete event simulation the \texttt{SimPy 3.0.10}\foot{https://simpy.readthedocs.io/en/latest/}{06.01.2017} package is used.
	\item The resulting data are interpreted and analyzed using Python and its plotting library: \texttt{Matplotlib 2.0.0}\foot{http://matplotlib.org/}{03.04.2017}.
	\item \texttt{Tensorflow 1.0}\foot{https://www.tensorflow.org/}{03.04.2017} is the library used for the neural networks modeling.
	\item Coding was done using \texttt{PyCharm 2017.1}\foot{https://www.jetbrains.com/pycharm/}{03.04.2017} as IDE for Python.
	\item For solving the mixed integer problems for batching policies \texttt{Gurobi 7.0.1}\foot{http://www.gurobi.com}{06.01.2017} was used.
\end{enumerate*}

\subsection{Discrete event simulation using SimPy}

SimPy is a Python process-based discrete-event simulation framework. It exploits Python generators according to which it models its processes.

Active components such as agents in a workflow are modeled as processes which live inside an environment and the interaction between them happens via events.

As previously mentioned, processes in SimPy are described by Python generators. During their lifetime they create events yield (Note that with the term \texttt{yield} here it is to be understood as Python's yield statements)\foot{https://docs.python.org/3.5/reference/simple_stmts.html\#the-yield-statement}{06.01.2017} them to the environment, which then wait until they are triggered. The important logic to understand here is how SimPy treats yielded events: when a process yields an event it gets suspended. From the suspended state a process gets resumed when the event actually occurs (or in SimPy's notation when it gets triggered).

SimPy offers a built-in event type called \texttt{Timeout}: events of this type are automatically triggered after a determined simulation time step. Consistency is asserted since a timeout event are created and called by called the appropriate method of the passed \texttt{Environment}.

\subsection{Analysis Environment}

The analysis environment consists in an object-oriented implementations of workflow process elements such as user task, starting, decision and end nodes which have been developed to allow the simulation framework to effectively run. This object-oriented exoskeleton implementation of the workflow elements can be seen depicted in \figref{fig:workflow_elements}.

\fig[\textwidth]{workflow_elements}{Workflow elements}{fig:workflow_elements}

The core elements of a workflow process (relevant for the simulation environment) are start nodes, user tasks, decision nodes and end nodes. Start events are used to indicate where and how a process starts and usually each process has only one such event \cite[p. 42]{Silver2011}. No distinction between trigger types is being made.

\subsubsection{Start event}

Start event objects require a simulation environment, a generation interval, an actions to follow array and its corresponding weights. The generation interval is generated in a three step process:
\begin{enumerate*}
 	\item before the simulation starts, a fixed service interval time unit $s$, number of users $n$ and an average system load $l$ are set. In contrast to Zeng's and Zhao's work, where the generation $\lambda$ interval follows a Poisson distribution \cite{Zeng2005} and is defined as shown in \equref{eq:generation_interval}, here the generation interval is a plain scalar value.
 	\item for a Poisson random exponential sampling of the generation rate, \texttt{NumPy}'s implementation of its exponential distribution is used\foot{https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.exponential.html}{06.01.2017}.
 \end{enumerate*}

\begin{equation}
\label{eq:generation_interval}
	\lambda = \frac{l n}{s}
\end{equation}

The actions to be followed are also defined in a two step process:
\begin{enumerate*}
	\item a per workflow process action pool is defined a priori in order to assert that tokens navigate the process in a ``semantically correct'' fashion
	\item then a weights vector is defined which assigns a weight to each possible action path to the actions pool.
\end{enumerate*}

Such an approach allows to fine tune how often tokens will follow a predefined path along the process in order to efficiently simulate and put under stress specific paths of the process.

In order to assert fairness among all simulation runs a master random state is assigned to the start event. This master random state is generated from the PCG family of random generators which exhibit peculiar characteristics, one amongst all is the possibility of ``jumping ahead'' in the state. By such means of ahead jumps it is possible to assign a fixed number of random yet consistent choices among all runs, since each generated token receives from the start event a ``jumped'' copy from the master state. For a better overview of the characteristics of the PCG random generators family consult its official outline \foot{http://www.pcg-random.org/}{03.04.2017}.

Even though tokens are generated infinitely, this process is controlled from the simulation environment where a discrete simulation time steps have to be set, as it can be seen from \lstref{lst:simulation_steps}.

This can be interpreted as that the whole simulation will persist for 100 time steps and it will then stop when the internal clock reaches 100. Please note that events that have been scheduled for time step 100 will not be processed. The logic is similar to a new environment where the clock is zero and no event have been processed yet.

\begin{lstlisting}[caption=Starting the simulation with discrete time steps,label=lst:simulation_steps,style=CustomPython]
    # "global" variables
    SIM_TIME = 100
    # runs simulation
    env.run(until=SIM_TIME)
\end{lstlisting}

\subsubsection{User task}
\label{subsec:user_task}

User task objects also require a simulation environment, a policy, a descriptive name, a service interval and task variability. Each user task has a unique \texttt{child} field which is being set prior to starting the simulation.

In regards to parameters service interval and task variability a detailed explanation is required. Both are used to randomly sample service rate intervals for each user active during the simulation. Zeng and Zhao in their work follow a two way process to generate such intervals \cite[p. 8]{Zeng2005}. However in this thesis' implementation a refined version of this process is used:
\begin{enumerate*}
	\item at initialization time, each user task receives a service rate $s$ and a task variability $t$ value
	\item inside the policy request method, for each user task a sample of an average processing time following an Erlang distribution (a special case of the gamma distribution) which takes as input parameters a shape $k$ and a scale $\theta$ is made. The shape value $k$, as the name suggests, defines the curve shape that the Erlang distribution will follow. In this case both values $k$ and $\theta$ are dynamically evaluated at runtime as $k=s/t$ and $\theta = t$. This concept is depicted in \lstref{lst:user_task}
	\item the average processing time becomes a unique value of each user task object and is used by each policy to sample each user's service time, again from an Erlang sampled pool as depicted in \lstref{lst:user_service_rate} and we shall call this value $p_j$
\end{enumerate*}

\lstref{lst:user_service_rate} gives a glimpse of the inner logic of how policies work. It is however out of scope for this section to cover this aspect and it is provided ``as is''. For each user eligible to work the assigned token, its service rate is sampled following the Erlang distribution. This time, the Erlang distribution takes as parameters the unique average processing time $p_j$ of user task $j$ and a value worker variability, which is a unique property of each policy, which we shall call $w$.

In order to sample a service rate $p_{ij}$ following the Erlang distribution for each user $i$, shape $k$ is evaluated as $k=p_j/w$ and scale as $\theta = w$ as it can be seen in \lstref{lst:user_service_rate}



\begin{lstlisting}[caption=User service rate sampling following an Erlang distribution,label=lst:user_service_rate,style=CustomPython]
    def request(self, user_task,token):
        average_processing_time = token.random_state.gamma(
            user_task.service_interval ** 2 / user_task.task_variability,
            user_task.task_variability / user_task.service_interval)

        policy_job.service_rate = [token.random_state.gamma(average_processing_time ** 2 / self.worker_variability,
                                                           self.worker_variability / average_processing_time) for
                                   _ in range(self.number_of_users)]
\end{lstlisting}

As previously mentioned, the Erlang distribution is a special case of the Gamma distribution where $k$ defines the shape of the curve. This distribution is better suited to model service rates since with an appropriate $k$ one can approximate a normal distribution without incurring in the aspect of having to manually reset negative values to one (thus loosing statistical generality). This is asserted by the formal definition of Erlang's support with $x \in [0,\infty)$.

\texttt{NumPy}'s implementation of its Erlang distribution is used\foot{https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.gamma.html}{06.01.2017}. \equref{eq:erlang_density} defines the probability density function of the Erlang's distribution with the alternative parametrization that uses $\mu$ instead of $\lambda$ as scale parameter, which is its reciprocal. This corresponds to the NumPy's implementation.

\begin{equation}
\label{eq:erlang_density}
	f(x;k,\mu) = \frac{x^{k-1} e^{-\frac{x}{\mu}}}{\mu^k (k-1)!} \text{ for } x,\mu \geq 0
\end{equation}

Each user task object has a claim token method, which takes tokens as input parameters and finally makes a call to its designed policy, passing the token. On this top level, without stepping into the single policies implementations, the logic is straightforward: start events generate tokens, user tasks that are direct children of start events claim the newly generated tokens, ask the designated policies to work the token assigned to them and finally, after a service interval timeout which corresponds to the user's specific service interval, they release the token. The logic can be seen in \lstref{lst:user_task}.

\begin{lstlisting}[caption=User task claim method,label=lst:user_task,style=CustomPython]
    def claim_token(self, token):
        token.worked_by(self)
        policy_job = self.policy.request(self, token)
        service_time = yield policy_job.request_event
        yield self.env.timeout(service_time)
        self.policy.release(policy_job)
\end{lstlisting}

\subsubsection{Policy}

Policies are a particular object that does not directly participate in the workflow processes, it merely serves a role as a general supervisor that has the whole overview of the process allowing it to operate on a more abstract level. 

The implementation of the policy objects can be seen in \figref{fig:policies_init}.

\fig[0.33\textwidth]{policies_init}{Policies abstract implementation}{fig:policies_init}

Each policy is a blueprint for the actual implementation of the policy itself. It holds minimal information such as a simulation environment, number of users and worker variability. The worker variability is a global parameter that is used for the service time generation as already explained in \subsecref{subsec:user_task}.

As a blueprint, each policy object defines two abstract method for requesting an optimal assignment for a specific token and for later releasing that token and effectively freeing the user that was busy working on it. Refer to \figref{fig:policy_met_att} for its implementation overview.

\fig[0.5\textwidth]{policy_met_att}{Policy methods and attributes}{fig:policy_met_att}

In its request method, each policy generates a policy job object, which is again an abstract implementation of a job that the policy will work in order to return an optimizes assignment to a user task. Each policy job requires a user task and a token object as initialization parameters in order to be uniquely identifiable inside the whole process. Moreover, each policy job object serves as a bookkeeping agent by storing and dumping useful information every time it status changes, such as arrival, assigned, started and finished times, assigned user and a list of service times for all available users. Refer to \figref{fig:policyjob_met_att} for its implementation overview.

\fig[0.5\textwidth]{policyjob_met_att}{Policy job methods and attributes}{fig:policyjob_met_att}

\section{Optimization Policies}
\label{sec:opt_policies}

Different types of policies have been implemented following the foundations laid by Zeng and Zhao as outlined in \subsecref{subsec:workflow}. In their work the authors investigate five ``role-resolution'' policies used for optimal task to user assignment \cite[p. 7]{Zeng2005}. Following a brief description of the five aforementioned policies:
\begin{enumerate*}
	\item A load balancing policy consists in assigning a task as soon as it arrives to a qualified worker with the shortest task queue at that moment. In this policy workers execute tasks assigned to them on a FIFO fashion. The authors call this policy the ``\gls{llqp}''.
	\item A policy that maintains a single queue being shared among all users is referred to the authors as ``\gls{sq}''.
	\item Another policy that maintains both a \gls{sq} among all users and each user having an own queue and transfers tasks from the former to the latter is called ``K-Batch'' policy. Transfer of tasks from the \gls{sq} to users is done using an optimal task assignment procedure as soon as the \gls{sq} reaches a critical batch size $K$.
	\item The following policies takes the ``K-Batch'' policy but reduces the individual queue size of each user to one. This means that the optimization problem is still being solved as soon as the \gls{sq} reaches the critical size $K$, however actual movement of tasks from the \gls{sq} to the individual user queue happens only when user $i$ is not busy \ie his individual queue is empty at simulation time $t$. This policy is called according to the authors as ``K-Batch-1''
	\item The last policy further simplifies the fourth by weakening the batch size constraint and reduces it to one. This means that the optimal task assignment procedure is executed immediately. This policy is referred by the authors as ``1-Batch-1''.
\end{enumerate*}

All batching policies require the solution of an optimization problem. The authors define this problem as ``minimizing the maximum flowtime given the dynamic availability of the workers'' and call it ``\gls{msa}''\cite[p. 7]{Zeng2005}. The authors define the task flowtime as the elapsed simulation time between task generation and its completion \cite{Zeng2005,Baker1974}. Formally \gls{msa} is formulated as follows:

\begin{align}
    \min_z \quad z\\
    \text{subject to: } \notag \\
    \sum_{i \in W} x_{ij} &= 1 \quad \forall j \in T\\
    a_i + \sum_{j \in T} x_{ij} p_{ij} &\leq z \quad \forall i \in W\\
    x_{ij} \quad \text{or} \quad x_{ij}&=1 \quad \forall i \in W, \forall j \in T
\end{align}

All variables definition still hold without loss of generality as defined by the authors \cite[pp. 5-7]{Zeng2005}.

The class inheritance structure of the policies implementation can be seen in \figref{fig:policies}.

\fig[\textwidth]{policies}{Policies class structure}{fig:policies}

The authors definition of the \gls{msa} problem is however a simplified version of the actual problem of ``minimizing the maximum task flowtime'' (MF) as defined by Baker \cite{Baker1974} with consideration of the dynamic arrival of tasks problem, defined by the authors as the \gls{dmf} problem \cite{Zeng2005}. The \gls{dmf} problem is formally defined by Zeng as follows:

\begin{align}
    \min_z \quad z\\
    \text{subject to: } \notag \\
    \sum_{i \in W} \sum_{k \in T} x_{ijk} &= 1 \quad \forall j \in T\\
    s_j &\geq r_j \quad \forall j \in T\\
    (x_{ijk} + x_{ij'(k+1)} - 1)(s_j + p_{ij}) &\leq s_{j'} \quad \forall i \in W, \forall k \in T, \forall j \in T, \forall j' \in T \label{eq:nonlinear_constraints_dmf}\\
    s_j + \sum_{i \in W} \sum_{k \in T} p_{ij} x_{ijk} - r_j &\leq z \quad \forall j \in T\\
    x_{ijk} = 0 \quad \text{or} \quad x_{ijk} = 1 &\quad \forall i \in W, \forall j \in T, \forall k \in T\\
    s_j &\geq 0
\end{align}

Again, all variables definition still hold without loss of generality as described by the authors \cite[p. 6]{Zeng2005}. As Zeng notes in his work, \equref{eq:nonlinear_constraints_dmf} contains nonlinear constraints but mentions that by adding auxiliary variables the aforementioned \gls{dmf} formulation can be effectively converted into a mixed integer program and thus solve it \cite[p. 6]{Zeng2005}. On this note Zeng argues that the application of the \gls{dmf} problem in practice poses some problems \cite{Zeng2005}. In this thesis however a conversion of the \gls{dmf} formulation proposed by Zeng is formulated in order to adequately solve the optimization problem. The formal definition of such optimization problem is called EDMF (which stands for extended \gls{dmf}) and is devised as follows:

\begin{align}
    \min_{z_{\text{max}}} \quad z_{\text{max}}\\
    \text{subject to: } \notag \\
    \sum_{i \in W} \sum_{k \in T} x_{ijk} &= 1 \quad \forall j \in T\\
    a_i + \sum_{j \in T} p_{ij} x_{ijk} &\leq z_{i*k} \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k=0\\
    z_{i*k-1} + \sum_{j \in T} p_{ij} x_{ijk} &\leq z_{i*k} \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k>0\\
    z_{i*k}+ \sum_{j \in T} w_j x_{ijk} &\leq z_{\text{max}} \quad \forall i \in W, \forall k \in T\\
    \sum_{j \in T} x_{ijk} &\leq 1 \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k=0\\
    \sum_{j \in T} x_{ijk} &\leq \sum_{j \in T} x_{ijk-1} \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k>0\\
    z_{i*k} &\geq 0 \quad \forall i \in W, \forall k \in T
\end{align}

This formulation clearly gets rid of the nonlinear constraints while still accounting for dynamical arrival of tasks, making thus the \gls{dmf} problem as defined by Zeng effectively solvable.

When considering the minimization of the maximum flowtime of a task inside a process, the EDMF formulation can be further simplified by adopting some assumptions about the order and sequence of tasks. Based on how the batching policies are implemented, the policy job objects to be worked by users are implicitly stored in a sorted fashion. This means that the $z$ helper variables defined for EDMF are not strictly necessary and thus can be compressed by \equref{eq:simplified_z_with_k}:

\begin{equation}
\label{eq:simplified_z_with_k}
	a_i + \sum_{t=1}^k \sum_j (p_{ij} + w_j I(t=k))x_{ijt}
\end{equation}

The whole concept consists in the introduction of an identity variable $I$ which is true only if task $j$ is currently being assigned as the $k$th task to user $i$, meaning that for this specific case also the waiting time for task $j$ has to be accounted for. For all other cases \ie $j<k$ the identity variable $I$ will not hold thus effectively zeroing the $w_j$ variable.

\figref{fig:edmf_task_assignment} depicts the potential scenario where three tasks are assigned to a specific user $i$ following a sequence where task 2, 3 and $j$ are assigned respectively as first, second and third tasks (thus respecting the $k$ notation outlined above them).

In order to calculate $z_{ijk}$, one has to consider also when user $i$ will actually be available to process his first task. This is depicted by the variable $a_i$, which summed together with the respective service times of user $i$ for task $j$ gives the complete work time user $i$ will require to process all tasks assigned to him.

\fig[0.75\textwidth]{dmf_problem}{EDMF Task Assignment}{fig:edmf_task_assignment}

Without further ado, the simplified formulation of the extended \gls{dmf} variant (called \gls{sdmf}) is the following:

\begin{align}
    \min_{z_{\text{max}}} \quad z_{\text{max}}\\
    \text{subject to: } \notag \\
    \sum_{i \in W} \sum_{k \in T} x_{ijk} &= 1 \quad \forall j \in T\\
    a_i + \sum_{t=1}^k \sum_j (p_{ij} + w_j I(t=k))x_{ijt} &\leq z_{\text{max}}\\
    \sum_{j \in T} x_{ijk} &\leq 1 \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k=0\\
    \sum_{j \in T} x_{ijk} &\leq \sum_{j \in T} x_{ijk-1} \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k>0
\end{align}

By comparing both formulation it is clear that \gls{sdmf} manages to simplify the mathematical formulation and relaxing the required amount of constraints while still attaining the same level of effectiveness. Please note, however, that this simplification is only possible because of the nature of the implementation.

Based on this approach and by further exploiting the implicit order implementation of task arrival in the global queues for both batching policies, it is possible to argue that the $k$ sequence indexing can be relaxed as well, thus even further simplifying the mathematical formulation and respectively the optimization problem size and computation costs.

The formulation of the \gls{dmf} problem by relaxing both the $z$ variables and $k$ indexes, it is possible to formulate the same \gls{dmf} problem as follows:

\begin{align}
    \min_{z_{\text{max}}} \quad z_{\text{max}}\\
    \text{subject to: } \notag \\
    \sum_{i \in W} x_{ij} &= 1 \quad \forall j \in T\\
    a_i + \sum_{k=1}^j (p_{ik} + w_k I(k=j))x_{ik} &\leq z_{\text{max}}
\end{align}

This formulation is colloquially called \gls{esdmf}.

This version is however only possible by the nature of its implementation. Since the both the global as well as the local queues are implemented as FIFO queues, it is possible to relax the ordering constraint from the mathematical formulation since it is already implicitly defined by the implementation.

Yet one last optimization of the method proposed by Zeng done in this thesis is a method that aims to optimally solve the assignment problem by changing its goal: minimize the service times by setting an upper bound on the maximum flowtime and is called \gls{st}. This method uses a two step process in order to optimally solve the assignment problem:
\begin{enumerate*}
	\item optimally solve by means of using on of the \gls{dmf} optimization methods previously outlined. This yields an upper bound for the maximum flowtime.
	\item use this upper bound as a constraint for the actual optimization in order to effectively optimize the problem for the minimal service time amongst users, jobs and their corresponding service time.
\end{enumerate*}

\begin{align}
    \min_z \quad \sum_{i \in W} \sum_{k \in T} z_{ik}\\
    \text{subject to: } \notag \\
    \sum_{i \in W} \sum_{k \in T} x_{ijk} &= 1 \quad \forall j \in T\\
    a_i + \sum_{j \in T} p_{ij} x_{ijk} - M(1 - \sum_{j \in T} x_{ijk}) &\leq z_{i*k} \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k=0\\
    z_{i*k-1} + \sum_{j \in T} p_{ij} x_{ijk} - M(1 - \sum_{j \in T} x_{ijk}) &\leq z_{i*k} \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k>0\\
    z_{i*k}+ \sum_{j \in T} w_j x_{ijk} &\leq z_{\text{max}} + \epsilon \quad \forall i \in W, \forall k \in T\\
    \sum_{j \in T} x_{ijk} &\leq 1 \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k=0\\
    \sum_{j \in T} x_{ijk} &\leq \sum_{j \in T} x_{ijk-1} \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k>0\\
    z_{i*k} &\geq 0 \quad \forall i \in W, \forall k \in T\\
    M &= \max_a a_i + \max_p \sum_{i \in W} \sum_{j \in T} p_{ij}\\
    \epsilon &= \num{1e-4}
\end{align}

\tabref{tab:big_oh_solvers} shows the computational complexity for the methods outlined in this chapter. The \gls{msa} method is the simplest solver and exhibits a linear complexity compared to the \gls{dmf} method proposed by Zeng. As it can be seen the methods implemented in this thesis, specifically the \gls{esdmf} method, both solves the \gls{dmf} method and does it by keeping the same linear complexity as the \gls{msa} method. The \gls{st} method proposed in this thesis exhibits a higher computational complexity but achieves a better optimization. This trade-off however requires a more in depth explanation which will follow in \secref{sec:op_results}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[!ht]
\centering
\begin{tabular}{@{}lc@{}}
\toprule
Solver & Computation Costs \\ \midrule
\gls{msa}    & $O(mn)$           \\
\gls{dmf}   & $O(mn^2)$         \\
\gls{sdmf}  & $O(mn^2)$         \\
\gls{esdmf}  & $O(mn)$           \\
\gls{st} 	   & $O(m^2n^2)$ \\	\bottomrule
\end{tabular}
\caption{Comparison of computational costs for different solvers}
\label{tab:big_oh_solvers}
\end{table}

\section{\glsentrylong{rl} Theory}

In this section the \gls{rl} approach used to solve the different role resolution problems is depicted. Initially a foundation basis in the required knowledge is depicted and afterwards the description of the analysis environment implementation is presented.

\subsection{\glsentrylong{rl} Definition}

\gls{rl} is a novel approach originated as a branch from the broader field of machine learning. It is an automated approach to understanding and automating learning and decision-making \cite[p. 15]{Sutton2017}. It distinguishes itself from other approaches by its novel focus on learning thanks to an agent which directly interacts with its environment, without the necessity of relying on training sets \cite[p. 15]{Sutton2017}.

The formal framework used by \gls{rl} defines the interaction between the so called learning agent and its environment by means of states, actions and rewards \cite[p. 15]{Sutton2017}.

Key concepts in the field of \gls{rl} are those of values and value functions which helps distinguish \gls{rl} methods from evolutionary methods which have to undergo scalar evaluations of entire policies \cite[p. 15]{Sutton2017}.

\subsection{Finite Markov Decision Processes}

\gls{rl} approaches learn by interacting with the environment in order to achieve a goal. The agent interacting with the environment does this in a sequence of discrete time steps, it performs actions (choices made by the agent), reaches then states (basis for making decisions) and eventually receives rewards (basis for evaluating the choices) \cite[p. 73]{Sutton2017}. Moreover, a policy is a stochastic rule that the agent relies upon to choose actions as a function of states \cite[p. 73]{Sutton2017}. Ultimately, the sole goal of the agent is to maximize the reward that it receives over time \cite[p. 73]{Sutton2017}.

Returns are modeled as functions of future rewards that an agents must maximize \cite[p. 73]{Sutton2017}. There exist two types of return functions which depend on the nature of the tasks and a discounting preference:
\begin{enumerate*}
	\item for episodic tasks a non discontinued approach is preferred while
	\item for continuous tasks, however, a discounted approach is better suited \cite[p. 73]{Sutton2017}.
\end{enumerate*}

\equref{eq:expected_return} defines the sum of the rewards received over time step $t$:

\begin{equation}
\label{eq:expected_return}
	G_t  \doteq R_{t+1} + R_{t+2} + R_{t+3} + \cdots R_{T}
\end{equation}

If we account for discounting, \equref{eq:expected_return} has to be slightly adapted by introducing a discounting factor $\gamma$ and can be found in \equref{eq:expected_discounted_return}:

\begin{equation}
\label{eq:expected_discounted_return}
	G_t  \doteq R_{t+1} + R_{t+2} + R_{t+3} + \cdots = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
\end{equation}

where $0 \leq \gamma \leq 1$.

An environment  with which one agent interacts, can satisfy a Markov property if the information contained at present effectively summarizes the past without affecting the capability of effectively predicting the future \cite[p. 73]{Sutton2017}. If the Markov property is satisfied, then this environment is called a Markov decision process (MDP) \cite[p. 73]{Sutton2017}.

Last but not least, value functions are used to assign each state or state-action pair an expected return based on the policy used by the agent \cite[p. 74]{Sutton2017}. Optimal value functions assign the highest achievable return by any policy to a state or state-action pair  and such policies, whose values are optimal, are called optimal policies \cite[p. 74]{Sutton2017}.

Optimal state-value functions $v_*$ are formally defined as follows:

\begin{equation}
	v_* (s) \doteq \max_\pi v_\pi (s)
\end{equation}

whereas optimal action-value functions $q_*$ are formally defined as follows:

\begin{equation}
	q_* (s,a) \doteq \max_\pi q_\pi (s,a)
\end{equation}

\subsection{Dynamic Programming}
\label{subsec:dp}

Dynamic programming (DP) is a set of ideas and algorithms that can be used to solve MDPs \cite[p. 95]{Sutton2017}. There are two approaches in dynamic programming for solving MDPs:
\begin{enumerate*}
	\item policy evaluations is the iterative computation of value functions of a given policy and
	\item policy improvement is the idea of computing an improved policy under the conditions of its given value functions \cite[p. 95]{Sutton2017}.
\end{enumerate*}

By combining these two approaches we obtain the two most notable DP methods \ie policy and value iteration \cite[p. 95]{Sutton2017}.

One captivating property of DP methods is the concept of bootstrapping: updating estimates of values of states by approximating the values of future states \cite[p. 96]{Sutton2017}.

\subsection{\glsentrylong{mc} Methods}
\label{subsec:mc}

\gls{mc} methods use experience in form of sample episodes in order to learn value functions and optimal policies \cite[p. 123]{Sutton2017}. This approach yields different advantages over the DP methods seen in \subsecref{subsec:dp}:
\begin{enumerate*}
	\item they do not need a model of the environment's dynamics as they learn the optimal solutions by merely interacting with the environment,
	\item since the learn from sample episodes, they are very well suited for simulation environments,
	\item it is efficient and surprisingly easy to use \gls{mc} methods to focus on smaller regions or subsets of a problem and
	\item \gls{mc} methods are more robust when it comes to violations of the Markov property since they do not bootstrap for updating their values \cite[p. 123]{Sutton2017}.
\end{enumerate*}

One of the drawbacks that \gls{mc} methods bring along is the concept of maintaining sufficient exploration: by always acting greedily, alternative states will never yield their returns thus potentially never learning that they might prove to be better \cite[p. 123]{Sutton2017}.

A \gls{mc} simplified method can be formally defined as follows:

\begin{equation}
\label{eq:mc_update}
	V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)]
\end{equation}

where $G_t$ is the discounted return function defined by \equref{eq:expected_discounted_return} and $\alpha$ is a constant step-size parameter \cite[p. 127]{Sutton2017}. \gls{mc} methods must wait until the end of one episode in order to evaluate the incremental value of $V(S_t)$ since only at that point in time $G_t$ is known \cite[p. 128]{Sutton2017}.

\subsection{\glsentrylong{td} Learning}
\label{subsec:td_learning}

\gls{td} are yet another set of learning methods for \gls{rl}. Compared to the \gls{mc} methods explained in \subsecref{subsec:mc}, \gls{td} methods do not need to wait all the way up to the end of an episode to actually learn, they only must wait until the next step \ie they can bootstrap \cite[p. 128]{Sutton2017}. When they reach time step $t+1$, they observe a reward $R_{t+1}$ which then use to estimate $V(S_{t+1})$ \cite[p. 128]{Sutton2017}. The simplest \gls{td} method, which is called $\gls{td}(0)$, is defined as follows:

\begin{equation}
\label{eq:td_update}
	V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]
\end{equation}

\gls{td} methods, same as \gls{mc} methods, are not  excluded from the sufficient exploration methods \cite[p. 147]{Sutton2017}. \gls{td} methods deal with this complication in two different ways:
\begin{enumerate*}
	\item \gls{onp} by using an algorithm called \gls{sarsa} and
	\item \gls{op} by using an algorithm called Q-learning \cite[p. 128]{Sutton2017}.
\end{enumerate*}

\subsection{\glsentrylong{onp} Prediction with Approximation}
\label{subsec:onpol_pred}

Up until now, the different methods presented are not suited for arbitrarily large state spaces. However, there exist solution to tackle such large state spaces: approximate solution methods. Under the assumption that one must always account for finite and limited computational resources, it is not feasible to find an optimal policy or value function, instead we have to settle for a good approximation of the solution \cite[p. 189]{Sutton2017}.

An essential characteristic for \gls{rl} algorithms venturing in the area of approximation is being able of generalization \ie using experience from a limited subset of the state space to effectively generalize and produce a valid approximation of a much larger subset  \cite[p. 189]{Sutton2017}. \gls{rl} methods are capable of achieving this by relying on supervised-learning function approximation which essentially use backups as training example \cite[p. 222]{Sutton2017}. Specifically, one brilliant set of methods are those using parametrized function approximation \ie the policy is parametrized by a weight vector $\theta$.

The parametrized functional form with weight vector $\theta$ can be used to write $\hat{v}(s,\theta) \approx v_\pi (s)$, which is the approximated value of state $s$ given weight vector $\theta$ \cite[p. 191]{Sutton2017}.

It is then clear that the weight vector $\theta$ has to be chosen wisely: this can be done by using variations of \gls{sgd} methods \cite[p. 223]{Sutton2017}. \gls{sgd} methods adjust the weight vector after each step by a tiny amount following the direction that would reduce the error the most:

\begin{align}
	\theta_{t+1} &\doteq \theta_t - \frac{1}{2} \alpha \nabla [v_\pi (S_t) - \hat{v} (S_t,\theta_t)]^2\\
	&= \theta_t + \alpha  [v_\pi (S_t) - \hat{v} (S_t,\theta_t)] \nabla \hat{v} (S_t,\theta_t)
\end{align}

where $\alpha$ is a positive step size parameter and $\nabla f(\theta)$:

\begin{equation}
	\nabla f(\theta) \doteq \left( \frac{\partial f(\theta)}{\partial \theta_1}, \frac{\partial f(\theta)}{\partial \theta_2}, \ldots, \frac{\partial f(\theta)}{\partial \theta_n} \right)^\top
\end{equation}

is the vector of partial derivatives with respect to $\theta$ \cite[p. 195]{Sutton2017}.

An exceptional case is linear methods for function approximation, where the approximate function $\hat{v} (\cdot ,\theta)$ is a linear function of the weight vector $\theta$ \cite[p. 198]{Sutton2017}. This means that for each state $s$ there is a corresponding vector of features $\phi (s) \doteq \left( \phi_1 (s), \phi_2 (s), \ldots, \phi_n (s) \right)^\top$ which has the same number of components as $\theta$ \cite[p. 198]{Sutton2017}. With this definition in mind, we can now formally define the state-\gls{vfa} as the inner product between $\theta$ and $\phi (s)$:

\begin{equation}
\label{eq:function_approximation_dot}
	\hat{v} (s,\theta) \doteq \theta^\top \phi (s) \doteq \sum_{i=1}^n \theta_i \phi_i (s)
\end{equation}

This simplified case of linear function approximation for state-value functions finally brings us to how we can use the \gls{sgd}:

\begin{equation}
\label{eq:sgd_linear}
	\nabla \hat{v} (s,\theta) = \phi (s)
\end{equation}

\equref{eq:sgd_linear} tells us that for the simple linear case the \gls{sgd} is nothing more than the corresponding features value  \cite[p. 199]{Sutton2017}.

\subsubsection{\glsentrylongpl{ann}}

Up to this point, we considered linear function approximation. \glspl{ann} can be used instead for nonlinear function approximation \cite[p. 199]{Sutton2017}. The simplest case of an \glspl{ann} is a single feedforward perceptron, meaning that it has only one hidden layer (\ie a layer that is neither an input nor an output layer) and that the \glspl{ann} at hand has no loops between its neurons, meaning that the output can not influence the input (compared to recurrent \glspl{ann}, in which the output indeed can influence the input) \cite[p. 216]{Sutton2017}. 

The connections between neurons inside an \glspl{ann} are called weights and the analogy to its human counterpart is how strong synaptic connections are \cite[p. 216]{Sutton2017}. Refer to \figref{fig:ann_1h} for a depiction of a single layer \glspl{ann}.

The key about solving nonlinearity with \glspl{ann} is how they apply nonlinear functions to the sum of their weights, this process is done by means of activation functions \cite[p. 216]{Sutton2017}. There are different types of activation functions that can be used but each one of the usually exhibits an ``S'' shape (\ie sigmoid), such as the sigmoid $\sigma(x) = \frac{1}{(1+e^{-x})}$, the $\tanh(x) = 2\sigma(2x)-1$ or the different classes of rectified linear unit (ReLU), which have become captivating in the last few years due to their peculiar characteristics such as $f(x) = \max(0,x)$ \cite[p. 216]{Sutton2017}.

\fig[0.5\textwidth]{ann_1h}{Single layer \glspl{ann}}{fig:ann_1h}

Even though single layer perceptrons are powerful enough to approximate nonlinearity, in the past years a development toward more complex \glspl{ann} with multiple hidden layers (\ie multi-layer perceptrons) has been on the rise \cite[p. 217]{Sutton2017}. These complex \glspl{ann} allow to solve many artificial intelligence tasks in a much more efficient way \cite{Bengio2009}. This area is called deep \gls{rl} and it has shed light on solutions that were never though possible before (for practical applications refer to Mnih's \cite{Mnih2015} and Silver's work \cite{Silver2016})\footnote{Note that the class of deep \glspl{ann} used in these works is a particular one called deep convolutional networks, which are specialized networks used for processing high dimensional data arranged in spatial arrays like images \cite[p. 219]{Sutton2017},\cite{Lecun1998}.} \cite{Bengio2009}. Refer to \figref{fig:ann_2h} for a depiction of a multi layer \glspl{ann}.

\fig[0.75\textwidth]{ann_2h}{Multi layer \glspl{ann}}{fig:ann_2h}

Despite appearing more complex, \glspl{ann} rely on a similar approach for learning (\ie updating their internal parameters, or in this case the whole network synaptic connections) based on the \gls{sgd} method outlined in \subsecref{subsec:onpol_pred} \cite[p. 217]{Sutton2017}. This algorithm is called backpropagation and consists of doing a forward pass in which the activation function of each neuron is computed and then a backward pass computes the partial derivatives for each synaptic connection \cite[p. 218]{Sutton2017}.

As any other function approximation method, overfitting can be a problem for \glspl{ann} as well and it is particularly present for deep \glspl{ann} \cite[p. 218]{Sutton2017}. There are different techniques that can be used in order to mitigate this effect, with the most prominent one being the dropout method outlined by Srivastava \cite{Srivastava2014}.

\subsection{\glsentrylong{onp} Control with Approximation}
\label{subsec:onpol_control}

Moving towards control with \gls{vfa}, we now focus on the approximation of the action-value function $\hat{q} (s,a,\theta) \approx q_* (s,a)$ \cite[p. 229]{Sutton2017}.

For the special case of the so called one-step \gls{sarsa} method, its gradient-descent update for the action-value function is defined as follows:

\begin{equation}
	\theta_{t+1} \doteq \theta_t + \alpha [ R_{t+1} + \gamma \hat{q} (S_{t+1}, A_{t+1}, \theta_t) - \hat{q} (S_t, A_t, \theta_t) ] \nabla \hat{q} (S_t, A_t, \theta_t)
\end{equation}

and this method has excellent good convergence properties towards optimality \cite[p. 230]{Sutton2017}.

\subsection{\glsentrylong{op} Methods with Approximation}
\label{subsec:offpol_methods}

When moving towards the field of \gls{op} learning, one of the biggest problems that one might incur in is the convergence problem: \gls{op} learning with approximation is considerably harder compared to its tabular counterpart \cite[p. 243]{Sutton2017}. \gls{op} learning defines two policies, $\pi$ and $\mu$, where the former is the value function we seek to learn based on the latter \cite[p. 243]{Sutton2017}.

A new aspect being introduced in \gls{op} learning is the importance sampling concept, formally defined as follow:

\begin{equation}
\label{eq:importance_sampling}
	\rho_t \doteq \frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}
\end{equation}

which can be used to ``warp the update distribution back to the \gls{onp} distribution, so that semi-gradient methods are guaranteed to converge.'' \cite[p. 243]{Sutton2017}.

During \gls{op} learning, both policies $\pi$ and $\mu$ are usually defined as full greedy and somewhat more exploratory $\epsilon$-greedy \cite[p. 243]{Sutton2017}.

For the purpose of this thesis, the focus has been put upon the episodic action value update algorithms, defined as follows:

\begin{align}
	\theta_{t+1} &\doteq \theta_t + \alpha \delta_t \nabla \hat{q} (S_t,A_t,\theta_t)\\
	\delta_t &\doteq R_{t+1} + \gamma \underbrace{\sum_a \pi (a|S_{t+1}) \hat{q} (S_{t+1},a,\theta_t)}_{\max_a \hat{q} (S_{t+1},a,\theta_t)} - \hat{q} (S_t,A_t,\theta_t)
\end{align}

what is important to note here, is that the episodic \gls{op} algorithm does not use importance sampling as defined by \equref{eq:importance_sampling} \cite[p. 244]{Sutton2017}. The authors state that this approach is clear for tabular methods but it is rather a ``judgment call'' for methods using approximation functions and deeper understanding of the theory of function approximation is needed \cite[p. 244]{Sutton2017}.

\subsection{\glsentrylong{pg} Methods}
\label{subsec:polgrad_methods}

Up until now all methods were based on the concept of learning values of actions and subsequently choosing the correct actions based on estimates, however, we now move our focus towards methods that actually learn a parametrized policy without needing value functions at all\footnote{Actor-critic methods are an exception, where a learned value function is used in combination with \gls{pg} as a baseline in order to lower variance.}  \cite[p. 265]{Sutton2017}. Parametrized policies work with probabilities that a specific action $a$ will be chosen at time $t$ if the agent finds itself in state $s$ at time $t$ with a weight vector $\theta$ \cite[p. 265]{Sutton2017}. For \gls{pg} methods it is crucial to learn the weight vector based on a performance measure $\eta(\theta)$ by trying to maximize and thus approximating the gradient ascent of $\eta$ as follows:

\begin{equation}
	\theta_{t+1} \doteq \theta_t + \alpha \widehat{\nabla \eta (\theta_t)}
\end{equation}

where $ \widehat{\nabla \eta (\theta_t)}$ is nothing else than a stochastic estimate that approximates the gradient of $\eta(\theta)$ \cite[p. 265]{Sutton2017}.

For discrete action spaces, a suitable solution consists in forming parametrized numerical preferences $h(s,a,\theta) \in \mathbb{R}$ \cite[p. 266]{Sutton2017}. This means that the best actions is given the highest probability according to a softmax distribution:

\begin{equation}
\label{eq:probabilistic_preferences}
	\pi(a|s,\theta) \doteq \frac{e^{h(s,a,\theta)}}{\sum_b e^{h(s,b,\theta)}}
\end{equation}

where $e \approx 2.71828$ \cite[p. 266]{Sutton2017}.

Moreover, the preferences can be, as previously mentioned:

\begin{equation}
\label{eq:dot_preferences}
	h(s,a,\theta) \doteq \theta^\top \phi (s,a)
\end{equation}

\ie simply linear in features \cite[p. 266]{Sutton2017}.

With these definitions in mind, one can formally define one of the very first \gls{mc} based \gls{pg} methods: REINFORCE \cite{Williams1992}.

Williams defines his REINFORCE algorithm by the following update function:

\begin{equation}
	\theta_{t+1} \doteq \theta_t + \alpha \gamma^t G_t \frac{\nabla_\theta \pi(A_t|S_t,\theta)}{\pi(A_t|S_t,\theta)}
\end{equation}

note that the vector $\frac{\nabla_\theta \pi(A_t|S_t,\theta)}{\pi(A_t|S_t,\theta)}$ is called eligibility vector and it is usually written in a more compact form of $\nabla \log \pi(A_t|S_t,\theta)$ by relying on the mathematical identity $\nabla \log x = \frac{\nabla x}{x}$ \cite[p. 271]{Sutton2017}.

For the REINFORCE algorithm, its eligibility vector is defined as follows:

\begin{equation}
	\nabla_\theta \log \pi (a|s,\theta) = \phi (s,a) - \sum_b \pi (b|s,\theta) \phi (s,b)
\end{equation}

and this method has solid convergence properties \cite[p. 271]{Sutton2017}.

\subsubsection{\glsentrylong{pg} with Baseline}

REINFORCE, however, being a method based on \gls{mc} it might exhibit high variance and prove relatively slow in its learning rate \cite[p. 271]{Sutton2017}. By introducing a baseline $b(s)$ to compare the action value:

\begin{equation}
	\theta_{t+1} \doteq \theta_t + \alpha (G_t - \overbrace{b(S_t)}^{\text{baseline}}) \frac{\nabla_\theta \pi(A_t|S_t,\theta)}{\pi(A_t|S_t,\theta)}
\end{equation}

one can achieve a positive effect towards diminishing variance of the update rule \cite[p. 271]{Sutton2017}.

\subsubsection{Actor-Critic \glsentrylong{pg}}

By introducing a base line, we have seen that variance can be lowered, however, the REINFORCE algorithm with a baseline is not a proper actor-critic method as its state-value function is only used as baseline and not as a critic \ie it is not used for bootstrapping \cite[p. 273]{Sutton2017}. By introducing bootstrapping we introduce bias and dependence of the quality of the approximated function, which in turn help to reduce variance and learn faster \cite[p. 273]{Sutton2017}. 

The only negative aspect still remaining is that \gls{pg} methods are still based on a full \gls{mc} update trajectory: this can be also mitigated by replacing the update function by \gls{td} learning approaches, such as those defined in \subsecref{subsec:td_learning} \cite[p. 273]{Sutton2017}.

The formal definition of a one-step actor-critic update method is depicted as follows:

\begin{equation}
	\theta_{t+1} \doteq \theta_t + \alpha(R_{t+1} + \overbrace{\gamma \hat{v}(S_{t+1},w)-\hat{v}(S_t,w)}^{\text{\gls{td} update}}) \frac{\nabla_\theta \pi(A_t|S_t,\theta)}{\pi(A_t|S_t,\theta)}
\end{equation}

and this is now a fully online implementation that executes updated after each newly visited state \cite[p. 274]{Sutton2017}. 

\section{\glsentrylong{rl} Policies}
\label{sec:rl_policies}

Analog to \secref{sec:opt_policies}, the same policies are considered where now different \gls{rl} methods and techniques are used to solve the assignment problem.

Different subsections are used in order to separate better the different approaches used for each type of policy:
\begin{enumerate*}
	\item Batch policy methods.
	\item \gls{llqp} policy methods.
	\item other policy methods that do not fit in any of the previous categories.
\end{enumerate*}

The two key concepts required in order to effectively apply \gls{rl} techniques in the domain of workflow processes and the optimal assignment of jobs to users are:
\begin{enumerate*}
	\item correctly defined states and actions spaces.
	\item precise rewards definition.
	\item effective update method for the policy's internal parameters. 
\end{enumerate*}

In the next subsections a distinction between prediction and update methods is outlined.

\subsection{Prediction and Control Methods}

As previously outlined in \subsecref{subsec:onpol_pred}, \subsecref{subsec:onpol_control}, \subsecref{subsec:offpol_methods} and \subsecref{subsec:polgrad_methods} there are different prediction and control methods that can be applied.

\subsubsection{\glsentrylong{vfa}}

As mentioned in \secref{sec:rl_policies}, it is crucial to correctly define the states and actions space for each problem. Each request that the policy receives generates a policy job object which is then passed in the internal evaluate method of the policy. Inside this method, for each policy job the state space $S$ is defined as a $n \times n+1$ matrix which contains all busy times of the potential candidates (\ie users) concatenated to the user's current service time for the job. Formally the state space is defined as depicted in \equref{eq:kbatch_sp}:

\begin{equation}
\label{eq:kbatch_sp}
	S_{n,n+1} = 
	\begin{bmatrix}
	a_1 & \cdots & a_1 \\
	a_2 & \cdots & a_2 \\
	\vdots & \ddots & \vdots \\
	p_{1,j} & \cdots & p_{i,j} \\
	\end{bmatrix}
\end{equation}

Since the possible actions are represented by the number of users, the state space is modeled such that for each possible actions a 1-D vector containing all busy times plus the service time of the user are present.

During the evaluation phase of a job the policy has to choose an action (\ie a user) by taking into account the current state space and its internal $\theta$ parameters. By using a state-\gls{vfa} as defined in \equref{eq:function_approximation_dot}, the policy evaluates the highest score for each possible user.

As an example, let us consider a snippet of how a K-Batch policy using a linear state-\gls{vfa} performs its choices during its greedy phase: it iterates over all possible actions and performs the dot product between the state space and the corresponding $\theta$ vector and then maximizes the returned $Q$ value. This approach can be seen in \lstref{lst:e_greedy} 

\begin{lstlisting}[caption=$\epsilon$-Greedy approach,label=lst:e_greedy,style=CustomPython]
    if self.greedy:
        action = max(range(self.number_of_users), key=lambda action: self.q(state_space, action))
    else:
        rnd = self.EPSILON_GREEDY_RANDOM_STATE.rand()
        if rnd < self.epsilon:
            action = self.EPSILON_GREEDY_RANDOM_STATE.randint(0, self.number_of_users)
        else:
            action = max(range(self.number_of_users), key=lambda action: self.q(state_space, action))
\end{lstlisting}

and its respective state-\gls{vfa} in \lstref{lst:value_f_approx}.

\begin{lstlisting}[caption=State-\gls{vfa},label=lst:value_f_approx,style=CustomPython]
    def q(self, states, action):
        features = self.features(states, action)
        q = np.dot(features[action], self.theta[action])
        return q
\end{lstlisting}
 
 $Q$ values are however only one part of the requirements set by \gls{rl} methods, the next crucial aspect is defining the reward function. Since \gls{rl} agents are able to back-propagate what they have learned from one episode and thus update their internal factors, correctly defining a reward is a must. Since the goal for our domain is minimizing the maximum flowtime (from now on this metric will be referred to as lateness) of a job, the reward itself corresponds to the lateness of a job during a specific task. This can be evaluated a priori since for each policy job we know its internal parameters required to calculate the lateness \ie busy time of user $i$ plus the service time of user $i$ for job $j$, or formally $a_i+p_{ij}$.

 The last definition required in order to effectively apply the update on the policy's internal parameters $\theta$ is defining the \gls{sgd} method as outlined by \equref{eq:sgd_linear}. This method will give us the direction in which we have to update our internal $\theta$ parameters during our chosen update method and it is nothing more than the features themselves. As an example, refer to \lstref{lst:features_definition} for the concrete implementation.

 \begin{lstlisting}[caption=Features definition,label=lst:features_definition,style=CustomPython]
    def features(self, states, action):
        features = np.zeros((self.number_of_users, self.number_of_users + 1))
        features[action] = states[action]
        return features
\end{lstlisting}

The features method outputs a matrix which has its values populated only for the actual chosen action. Let us assume our policy has chosen user 1 out of 2 possible users, then the state space looks as defined by \equref{eq:kbatch_sp_ex}:

\begin{equation}
\label{eq:kbatch_sp_ex}
	S_{2,3} = 
	\begin{bmatrix}
	a_1 & a_1 \\
	a_2 & a_2 \\
	p_{1,j} & p_{2,j} \\
	\end{bmatrix}
\end{equation}

and its features vector looks as defined by \equref{eq:kbatch_features_ex}:

\begin{equation}
\label{eq:kbatch_features_ex}
	F_{2,3} = 
	\begin{bmatrix}
	a_1 & 0 \\
	a_2 & 0 \\
	p_{1,j} & 0 \\
	\end{bmatrix}
\end{equation}

\subsubsection{\glsentrylong{pg}}

With \gls{pg} methods the approach on how an action is chosen is shifted. Instead of maximizing a $Q$ value through internal $\theta$ parameters in order to choose the ``best greedy'' action, we now have probabilistic choices. As already outlined in \subsecref{subsec:polgrad_methods}, having a probabilistic policy $\pi$ means that the best action is now chosen according to the highest probability which follows a softmax distribution as defined in \equref{eq:probabilistic_preferences} and its implementation can be seen in \lstref{lst:softmax_probabilities}.

\begin{lstlisting}[caption=Softmax distribution of preferences probabilities,label=lst:softmax_probabilities,style=CustomPython]
    def policy_probabilities(self, busy_times):
        probabilities = [None] * self.number_of_users
        for action in range(self.number_of_users):
            probabilities[action] = np.exp(np.dot(self.features(busy_times, action), self.theta)) / sum(
                np.exp(np.dot(self.features(busy_times, a), self.theta)) for a in range(self.number_of_users))
        return probabilities
\end{lstlisting}

The policy probabilities method takes as input parameter the current state space and computes for each user its probability according to the current internal $\theta$ parameter as defined in \equref{eq:dot_preferences}. The result of this method is a probabilities (or weights) 1-D vector corresponding to a preference to assign a job to a specific user, where the index of the vector corresponds to the user and the value to its preference. Based on this preferences vector, the policy then computes a weighted random choice among all users, as can be seen in \lstref{lst:prob_user_choice}.

\begin{lstlisting}[caption=Probabilistic user choice,label=lst:prob_user_choice,style=CustomPython]
	chosen_action = self.RANDOM_STATE_PROBABILITIES.choice(self.number_of_users, p=probabilities)
\end{lstlisting}

\subsubsection{\glsentrylongpl{ann} as Function Approximation}

Up to this point we have used linear functions for the approximation of the $Q$ value for the different policies. As mentioned in \subsecref{subsec:onpol_pred}, \glspl{ann} can be used for nonlinear function approximation. The assignment problem poses its very well for this kind of application, in which we model our input layer as a 1-D vector containing all required information such as waiting time $w$ of job $j$, service time $p_{ij}$ of user $i$ for job $j$ and busy time $a_i$ of user $i$. By following a \gls{pg} approach, we can categorize the output layer of our \glspl{ann} using a softmax categorization function, mapping the preferences of job $j$ to user $i$ assignment as probabilities. \lstref{lst:ann_1h} show the modeling of a single layer perceptron in \gls{tf}: one hidden layer connects the state space (\ie input to the \glspl{ann}) together with its weights and biases, creates an activation function and maps the prediction layer (\ie output) with a softmax classification.

\begin{lstlisting}[caption=Modeling of a single perceptron in \gls{tf},label=lst:ann_1h,style=CustomPython]
	with tf.name_scope("neural_network"):
    	layer_1 = tf.add(tf.matmul(state_space_input, weights['h1']), biases['b1'])
    	layer_1 = tf.nn.elu(layer_1)
    	pred = [tf.add(tf.matmul(layer_1, weights['out'][b]), biases['out'][b]) for b in range(batch_input)]
    	probabilities = [tf.nn.softmax(pred[b]) for b in range(batch_input)]
\end{lstlisting}

In order to update the \glspl{ann}, a backpropagation has to take place. Such an update can both be made following a \gls{mc} or \gls{td} approach (refer to \subsecref{subsec:update_methods} for a detailed distinction between these two update methods). As outlined in \subsecref{subsec:onpol_pred}, we follow a \gls{sgd} approach in which we update all the synaptic connections by computing the partial derivatives of all the weights. \lstref{lst:mc_backpropagation} shows how the backpropagation for an \glspl{ann} following a \gls{mc} update method is done.

\begin{lstlisting}[caption=Backpropagation algorithm following a \gls{mc} update approach,label=lst:mc_backpropagation,style=CustomPython]
    def train(self):
        for t, (state, output, choices) in enumerate(self.history):
            disc_rewards = self.discount_rewards(t)
            tmp_choices = [choice for choice in choices if choice is not None]
            for job_index, chosen_user in enumerate(tmp_choices):
                prob_value = output[job_index].flatten()[chosen_user]
                reward = disc_rewards[job_index]
                factor = reward / prob_value
                grad_input = np.zeros((self.number_of_users, 1))
                grad_input[chosen_user] = 1.0
                self.sess.run(self.apply[job_index], {self.state_space_input: state, self.gradient_input: grad_input, self.factor_input: factor})
\end{lstlisting}

\subsection{Update Methods}
\label{subsec:update_methods}

As outlined in \subsecref{subsec:mc} and \subsecref{subsec:td_learning}, there are mainly two different methods to update the policy's internal $\theta$ parameters \ie \gls{mc} and \gls{td}. Let us take the example outlined by Sutton and Barto of leaving the office and getting home and the respective updates proposed by the two update methods \cite[p. 130]{Sutton2017}. \figref{fig:mc_td} shows the graphical updates proposed by the two update methods.

\fig[\textwidth]{mc_td}{\gls{mc} and \gls{td} proposed updates comparison (own plot based on Sutton and Barto)}{fig:mc_td}

As it can be clearly seen, the main deference lays in when the actual updating takes place. On one hand, the \gls{mc} method needs to reach the end of an entire episode (\ie here it consists of actually arriving home) in order to fully back-propagate its learned value and update the $\theta$ parameters. On the other hand, \gls{td} is much more flexible and robust since it executes its updates at each time step, hence its name: \gls{td}.
For a formal overview of the difference between the two update methods, refer to \equref{eq:mc_update} for the \gls{mc} update and to \equref{eq:td_update} for the \gls{td} update.
For the case at hand, this means that training the policies has to be done in a different fashion for the two update method: while \gls{td} based policies can be updated ``on-the-fly'', \gls{mc} methods require batch training sessions (\ie episodes) at the end of which they can effectively learn and update their internal $\theta$ parameters to be used for the next episode. Not only the training approach is different, but the logic of the policy itself is also different: for \gls{td} based policies, the update method is being called internally since the policy knows its temporal steps, while for \gls{mc} based policies the policy itself can not know a priori when an episode will finish and thus must relay on an ``artificial'' definition of such. The overall overhead is also different since \gls{mc} based policies have to keep track of their whole episode history which usually is composed of the state space, chosen action and reward at time step $t$.

Sutton and Barto outline a qualitative comparison between both update methods which can be found summarized in table \tabref{tab:mc_td_comp}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[!ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Characteristic & \gls{mc}                     & \gls{td}                               \\ \midrule
Bootstrap      & No                     & Yes                              \\
Update Time    & End of episode         & Each time step                   \\
Discount       & Required               & Not required                     \\
Convergance    & Good                   & Very Good                        \\
Learning Rate  & Slow for long episodes & Very fast even for long episodes \\ \bottomrule
\end{tabular}
\caption{Qualitative comparison between \gls{mc} and \gls{td} update methods \cite[p. 130]{Sutton2017}}
\label{tab:mc_td_comp}
\end{table}

\subsection{Batch Size Emulation}
\label{subsec:batch_size_emulation}

Correctly defined state spaces is a crucial requirement for effective \gls{rl} methods. \gls{llqp} policies are relatively easy to be modeled, on the other hand policies with batch sizes require a more meticulous consideration. By introducing a batch size that retains jobs in its global queue (\ie all batch sizes $K>1$), not only the job-to-user assignment plays a role, be the ordering of the assignment influences greatly the final outcome as well. Let us consider a simple case with users $i=2$ and jobs $j=3$ at time step $t$. \tabref{tab:users_service_times_example} summarizes the service times $p_{ij}$ in time units $t$ of both users for all three jobs.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[!ht]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
User   & Job 1 & Job 2 & Job 3 \\ \midrule
User 1 & $1$     & $2$     & $3$     \\
User 2 & $4$     & $5$     & $6$     \\ \bottomrule
\end{tabular}
\caption{Service times of both users for all three jobs}
\label{tab:users_service_times_example}
\end{table}

It is clear that the ordering of the jobs assigned has an impact on the final outcome. \figref{fig:user_job_assignment_order} outlines a possible conformation where user 1 receives job 1 while user 2 gets assigned to jobs 2 and 3 respectively. In this case, job 1 is started at $t$ and is finished at $t+1$, job 2 is started at $t$ and is finished at $t+5$ and job 3 is started at $t+5$ and is finished at $t+11$. The respective lateness per job is: $1$ for job 1, $5$ for job 2 and $6$ for job $3$.

By changing the assignment order (refer to \figref{fig:user_job_assignment_order2} for a graphical representation), the final outcome changes as well: In this case, job 1 is started at $t+2$ and is finished at $t+3$, job 2 is started at $t$ and is finished at $t+2$ and job 3 is started at $t$ and is finished at $t+6$. The respective lateness per job is: $1$ for job 1, $2$ for job 2 and $6$ for job $3$. By merely changing the assignment order a reduction of $40\%$ in lateness is observed by job 2.

\begin{figure}[!ht]
	\centering
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{img/user_job_assignment_order}
		\caption{User 1 receives job 1 while user 2 receives jobs 2 and 3}
		\label{fig:user_job_assignment_order}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{img/user_job_assignment_order2}
		\caption{User 1 receives jobs 2 and 1 while user 2 receives job 3}
		\label{fig:user_job_assignment_order2}
	\end{minipage}
\end{figure}

\subsecref{subsec:rl_others} outlines three different types of policies (refer to \tabref{tab:rl_others_policies_overview} for a detailed explanation of the policies) that take into account the previously outlined job assignment ordering principle and exploit it in their state space modeling. This is done by means of integrating an additional parameter $B$ that defines how many jobs have to be considered when trying to optimally assign jobs lying in the global queue to users. This is done by listing all possible combinations by accounting for the job order as well. Refer to \lstref{lst:wz_combinations} for the actual implementation.

\begin{lstlisting}[caption=State space modeling by considering $B$ jobs from the global queue and integrating all possible combinations,label=lst:wz_combinations,style=CustomPython]
	combinations = list(itertools.product(range(self.number_of_users), repeat=self.wait_size))
    for i, combination in enumerate(combinations):
        state_space[i] = a + [p[user_index][job_index] for job_index, user_index in enumerate(combination)]
\end{lstlisting}

This approach effectively simulates batch policies with batch sizes $K>1$.

\section{Hypothesis}
\section{Data}

\chapter{Empirical Analysis}
\label{ch:empirical_analysis}

\section{Methodology}

In order to consistently and fairly evaluate all policies with the methods defined in the previous chapters, the following methodology was put in place:
\begin{enumerate*}
	\item Each policy has its own simulation script that initialized a process that uses the predefined policy as means to optimally assign jobs to tasks
	\item Parameters are centrally defined.
	\item Different \glspl{kpi} have been defined which are used to assert the efficiency of one policy against one another.
\end{enumerate*}

\subsection{Simulation script}

Each simulation script is the abstract element that import all required dependencies, initializes the SimPy simulation environment, the statistics file into which the policy dumps all data on runtime, the policy object itself to be used for the assignment and the workflow process to be used. 

\begin{lstlisting}[caption=Example of the structure of a simulation script. Here for the K-Batch policy using the \gls{dmf} solver,label=lst:simulation_script,style=CustomPython]
	import simpy
	from evaluation.statistics import calculate_statistics
	from evaluation.subplot_evolution import evolution
	from policies.optimization.batch.k_batch import K_BATCH
	from simulations import *
	from solvers.dmf_solver import dmf

	policy_name = "{}_BATCH_DMF_NU{}_GI{}_SIM{}".format(BATCH_SIZE,NUMBER_OF_USERS, GENERATION_INTERVAL, SEED, SIM_TIME)

	env = simpy.Environment()

	file_policy = create_files("{}.csv".format(policy_name))

	policy = K_BATCH(env, NUMBER_OF_USERS, WORKER_VARIABILITY, file_policy,BATCH_SIZE, dmf)

	start_event = acquisition_process(env,policy,1,GENERATION_INTERVAL,False,None,None,None)

	env.process(start_event.generate_tokens())

	env.run(until=SIM_TIME)

	file_policy.close()

	calculate_statistics(file_policy.name, outfile=True)

	evolution(file_policy.name, outfile=True)
\end{lstlisting}

The script initialized the chosen workflow process and then calls the tokens generation method of the start event. Eventually the whole simulation is started by calling the run method of the SimPy environment.

\subsection{Workflow Process Modeling}

Two different types of processes have been defined:
\begin{enumerate*}
	\item Consisting of only one user task.
	\item A complex workflow process that is modeled against an acquisition process used in the real estate field for the acquisition of real estate properties.
\end{enumerate*}

\figref{fig:simple_process} illustrates the simple process.

\fig[0.5\textwidth]{simple_process}{Simple workflow process consisting of only one user task}{fig:simple_process}

\figref{fig:acquisition_process} illustrates the complex acquisition process.

\fig[\textwidth]{acquisition_process}{Acquisition workflow process consisting of multiple user tasks and decision nodes}{fig:acquisition_process}

\subsection{Central Simulation and Process Parameters Definition}

One key aspect in order to assert fairness across policies while simulated is to centrally define all parameters. \lstref{lst:central_parameters} shows the key central parameters defined as global variables.

\begin{lstlisting}[caption=Central parameters definition that ensures fairness across simulation runs,label=lst:central_parameters,style=CustomPython]
	NUMBER_OF_USERS = 2
	SERVICE_INTERVAL = 1
	GENERATION_INTERVAL = 3
	SIM_TIME = 100
	BATCH_SIZE = 1
	TASK_VARIABILITY = 0.2 * SERVICE_INTERVAL
	WORKER_VARIABILITY = 0.2 * SERVICE_INTERVAL
	SEED = 2
\end{lstlisting}

\subsection{\glsentryshortpl{kpi} for Asserting Policy's Efficacy and Data Visualization}

Based on Pinedo's and Zeng's definitions in their work, different \glspl{kpi} have been defined to assert the efficacy of a policy, such as lateness, waiting time, service time, number of tokens completed, user loads and system load \cite{Pinedo2008},\cite{Zeng2005}. Following the formal definitions of the per token $j$ \glspl{kpi} in respect to lateness $L_j$, wait time $w_j$, service time $p_{ij}$ of assigned user $i$, arrival time $A_j$, assignment time $a_j$, start time $S_j$ and finish time $F_j$.

\begin{align}
	L_j&=F_j-A_j \label{eq:lateness}\\
	w_j&=S_j-A_j \\
	p_{ij}&=F_j-S_j
\end{align}

Moreover, if we account for simulation time $T$, load $l_i$ of user $i$ is defined as the sum of all service times of tokens that have been assigned to him during the simulation divided by the total simulation time $T$, or formally:

\begin{equation}
	l_i=\frac{\sum_j p_{ij}}{T}
\end{equation}

and thus the average system load $\overline{l}$ over all users participating $n$ is defined as the average across all user's loads \ie

\begin{equation}
	\overline{l} = \frac{\sum_i l_i}{n}
\end{equation}

A summary plot with all \glspl{kpi} is done for each simulation script. \figref{fig:kpi_plot} shows an example of how this summary looks like.

\fig[\textwidth]{3_BATCH_MSA_NU2_GI3_SIM50_KPI}{\glspl{kpi}  summary plot for a 3-Batch policy using the \gls{msa} solver, with two users, generation interval set to three and simulation time $T$ set to 50}{fig:kpi_plot}

Additionally, for a more in depth visualization of a policy's performance, an evolution plot is also necessary. All types of policies share a common queues configuration, with a single global queue and a user specific queue. Each policy defines the maximal threshold a specific queue can reach. For a detailed explanation of the queues conformation refer to \secref{sec:opt_policies}.

The evolution plot shows the state change for the policy being analyzed by plotting the flow of a token across different user tasks. \figref{fig:evo_plot} shows such an example.

\fig[\textwidth]{3_BATCH_MSA_NU2_GI3_SIM50_EVO}{Evolution plot for a 3-Batch policy using the \gls{msa} solver, with two users, generation interval set to three and simulation time $T$ set to 50}{fig:evo_plot}

\clearpage

\section{Optimization}
\label{sec:op_results}

This section focuses on the results of the different types of policies using the optimization solvers outlined in \secref{sec:opt_policies}. All simulations have been tested with different combination of global variables \ie number of users, service interval, generation interval, length of simulation time, batch size (where it applies), task variability, worker variability and random state seed (where it applies). For ease of reading purposes, the global variables have been set to the following parameters according to the default column in \tabref{tab:global_parameters_sim}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[!ht]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
Variable            & Default     & Valid Range \\ \midrule
Number of Users     & $3$           & $1-\infty$      \\
Service Interval    & $1$           & $1-\infty$      \\
Generation Interval & $3$           & $1-\infty$      \\
Simulation Time     & $50$          & $1-\infty$      \\
Batch Size          & $3$           & $1-\infty$      \\
Task Varaibility    & $20\%$        & $0\%-100\%$      \\
Worker Variability  & $20\%$        & $0\%-100\%$      \\
Random State Seed   & $2$           & $\emptyset-\infty$      \\
Workflow Process    & Acquisition & Acquisition, Simple      \\ \bottomrule
\end{tabular}
\caption{Global Parameters for Simulation}
\label{tab:global_parameters_sim}
\end{table}

\subsection{Comparison with Existing Literature}

Zeng outlines in his work how different global parameters configurations and policy usage can affect \glspl{kpi} \cite[pp. 18-22]{Zeng2005}. He summarizes his key findings as follows:
\begin{enumerate*}
	\item Usage of batch optimization should be done only with medium to high system load \cite[p. 24]{Zeng2005}.
	\item Batch optimization policies without a fixed batch size, such as 1-Batch-1 yield best results \cite[p. 24]{Zeng2005}.
\end{enumerate*}

In order to assert the validity of the interpretation of Zeng's works and all subsequent derivative policies a comparison with similar configurations has been made for all five optimization policies. Zeng's main efficiency parameter is defined as the maximum flowtime or ``In business terms, maximum flowtime represents the guaranteed response time across tasks, indicating the quality of services'' \cite[p. 17]{Zeng2005}. In this study, the comparable parameter used to evaluate a policy's efficiency is called lateness and has been previously defined in \equref{eq:lateness}.

In regards to lateness, \figref{fig:opt_kpis_comp} shows that akin results are obtained.

\fig[\textwidth]{opt_kpis_comp}{\glspl{kpi}  comparison for different optimization policies using the \gls{msa} solver for batch policies}{fig:opt_kpis_comp}  

The simulation have been run with the parameters outlined in tab by using the same solver used by Zeng: \gls{msa}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[!ht]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Variable            & Value     \\ \midrule
Number of Users     & $5$                 \\
Service Interval    & $1$              \\
Generation Interval & $2$                 \\
Simulation Time     & $50$              \\
Batch Size          & 5 (1 for 1-Batch-1)               \\
Task Varaibility    & $20\%$             \\
Worker Variability  & $20\%$              \\
Random State Seed   & $2$                 \\
Workflow Process    & Acquisition       \\ \bottomrule
\end{tabular}
\caption{Global Parameters for Optimization Policies \glspl{kpi} Comparison}
\label{tab:global_parameters_kpis_comp}
\end{table}

By running the same simulations with the optimization solver implemented for this thesis (\ie \gls{st}, refer to \secref{sec:opt_policies}), ceteris paribus, the summarized \glspl{kpi} amongst all optimization policies can be seen in \figref{fig:opt_st_kpis_comp}.

\fig[\textwidth]{opt_st_kpis_comp}{\glspl{kpi}  comparison for different optimization policies using the \gls{st} solver for batch policies}{fig:opt_st_kpis_comp}

The percent reduction for both batch policies with a higher batch size is tiny, but when considering the reduction between the 1-Batch-1 policy with \gls{msa} and \gls{st}, a wealthy reduction is present for all \glspl{kpi}. For a detailed overview of the overall reductions of the \gls{st} against the \gls{msa} solver refer to \figref{fig:opt_kpis_comp_gain}.

\fig[\textwidth]{opt_kpis_comp_gain}{\glspl{kpi}  reduction comparison between the \gls{msa} and the \gls{st} solvers for different batch policies}{fig:opt_kpis_comp_gain}

Astonishing reductions have been observed for the 1-Batch-1 policy, which is indeed the most efficient policy as mentioned by Zeng (for a detailed comparison of how different batch sizes affect the policy's \glspl{kpi} refer to \subsecref{subsec:kbatch_bscomp_app} and \subsecref{subsec:kbatchone_bscomp_app}) \cite[p. 24]{Zeng2005}. \tabref{tab:opt_kpis_comp_gain} summarizes these values.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[!ht]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
\gls{kpi}                 & Reduction (in \%) \\ \midrule
Lateness            & $38.28$             \\
Wait Time           & $528.14$             \\
Service Time        & $19.75$             \\
Average System Load & $17.14$             \\ \bottomrule
\end{tabular}
\caption{Reduction (in \%) across all \glspl{kpi} of the \gls{st} against the \gls{msa} solver}
\label{tab:opt_kpis_comp_gain}
\end{table}

\clearpage

\section{\glsentrylong{rl}}
\label{sec:rl_results}

This section focuses on the results obtained with the \gls{rl} methods outlined in \secref{sec:rl_policies}. A more in-depth review of the different policies is required, thus a finer subdivision has been made in different subsections per policy type: \subsecref{subsec:rl_batch} focuses on batch policies, \subsecref{subsec:rl_llqp} focuses on \gls{llqp} policies and \subsecref{subsec:rl_others} focuses on all remaining policies that do not fit in either of the previous categories.

In order to maintain fairness amongst \gls{rl} training methods, all required parameters are globally set and equal across all simulation scripts and can be found summarized in \tabref{tab:global_rl_params}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[!ht]
\centering
\begin{tabular}{@{}ll@{}}
\toprule
Parameter        & Value  \\ \midrule
$\gamma$            & 0.5    \\
$\alpha$            & 0.0001 \\
$\epsilon$          & 0.1    \\
\gls{mc} epochs        & 1000   \\
\gls{td} training time & 1000   \\ \bottomrule
\end{tabular}
\caption{Global \gls{rl} parameters}
\label{tab:global_rl_params}
\end{table}

Comparisons will be made, where not otherwise stated, with the corresponding optimization policy simulated under the same conditions.

\subsection{Batch}
\label{subsec:rl_batch}

Five different batch policies with \gls{rl} have been developed. \tabref{tab:rl_batch_policies_overview} gives an overview.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[!ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllll@{}}
\toprule
Technical Name          & Policy Type & Update Method & $Q$ Value Method               & Other Characteristics     \\ \midrule
k\_batch\_mc\_vfa       & 1-Batch     & \gls{mc}            & \gls{vfa} & None                      \\
k\_batch\_mc\_vfa\_op   & 1-Batch     & \gls{mc}            & \gls{vfa} & \gls{op}                \\
k\_batch\_mc\_vfa\_opep & 1-Batch     & \gls{mc}            & \gls{vfa} & $\epsilon$-Greedy \gls{op} \\
k\_batch\_td\_vfa\_op   & 1-Batch     & \gls{td}            & \gls{vfa} & \gls{op}                \\
k\_batchone\_td\_vfa\_op   & 1-Batch-1     & \gls{td}            & \gls{vfa} & \gls{op}                \\ \bottomrule
\end{tabular}%
}
\caption{Overview of developed batch policies with \gls{rl}}
\label{tab:rl_batch_policies_overview}
\end{table}

\tabref{tab:rl_batch_kpis_comp_gain} shows the summarized results. For the detailed results refer to \subsecref{subsec:onebatch_rl_msa_comp_app} for 1-Batch respectively to \subsecref{subsec:onebatchone_rl_msa_comp_app} for 1-Batch-1.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[!ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llllll@{}}
\toprule
gls{kpi}                 & k\_batch\_mc\_vfa & k\_batch\_mc\_vfa\_op & k\_batch\_mc\_vfa\_opep & k\_batch\_td\_vfa\_op & k\_batchone\_td\_vfa\_op \\ \midrule
Lateness            & $32.96$                  & $33.88$                      & $32.75$                        & $33.71$                      & $20.33$                         \\
Wait Time           & $230.3$                  & $226.58$                      & $167.28$                        & $248.98$                      & $44.4$                         \\
Service Time        & $12.07$                  & $13.06$                      & $14.71$                        & $12.11$                      & $14.14$                         \\
Average System Load & $11.74$                  & $12.75$                      & $14.4$                        & $11.8$                      & $13.81$                         \\ \bottomrule
\end{tabular}%
}
\caption{Reduction (in \%) across all \glspl{kpi} of the batch policies with \gls{rl} against the \gls{msa} solver}
\label{tab:rl_batch_kpis_comp_gain}
\end{table}

\subsection{\glsentrylong{llqp}}
\label{subsec:rl_llqp}

Three different \gls{llqp} policies with \gls{rl} have been developed. \tabref{tab:rl_llqp_policies_overview} gives an overview. Other policies have been implemented for evaluating different \gls{rl} methods, however they are not considered for the final evaluation. These policies can be seen in \tabref{tab:rl_llqp_add_policies_overview}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[!ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllll@{}}
\toprule
Technical Name          & Policy Type & Update Method & $Q$ Value Method               & Other Characteristics     \\ \midrule
llqp\_mc\_vfa\_op       & \gls{llqp}     & \gls{mc}            & \gls{vfa} & \gls{op}                      \\
llqp\_td\_vfa\_op   & \gls{llqp}     & \gls{td}            & \gls{vfa} & \gls{op}                \\
llqp\_td\_tf\_op   & \gls{llqp}     & \gls{td}            & \glspl{ann} & \gls{op}                \\ \bottomrule
\end{tabular}%
}
\caption{Overview of developed \gls{llqp} policies with \gls{rl}}
\label{tab:rl_llqp_policies_overview}
\end{table}

\tabref{tab:rl_llqp_kpis_comp_gain} shows the summarized results. For the detailed results refer to \subsecref{subsec:llqp_rl_msa_comp_app}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[!ht]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
gls{kpi}                 & llqp\_mc\_vfa\_op & llqp\_td\_vfa\_op & llqp\_td\_tf\_op \\ \midrule
Lateness            & $0.18$                  & $0.21$                  & $0.81$                 \\
Wait Time           & $2.98$                  & $0.81$                  & $10.8$                 \\
Service Time        & $-0.14$                  & $0.14$                  & $-0.3$                 \\
Average System Load & $-0.15$                  & $0.19$                  & $-0.3$                 \\ \bottomrule
\end{tabular}
\caption{Reduction (in \%) across all \glspl{kpi} of the \gls{llqp} policies with \gls{rl} against the \gls{msa} solver}
\label{tab:rl_llqp_kpis_comp_gain}
\end{table}

\subsection{Others}
\label{subsec:rl_others}

Three different additional policies with \gls{rl} have been developed which have been used to fully emulate the behavior of K-Batch and 1-Batch-1 (as explained in \subsecref{subsec:batch_size_emulation}). \tabref{tab:rl_others_policies_overview} gives an overview.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[!ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllll@{}}
\toprule
Technical Name          & Policy Type & Update Method & $Q$ Value Method               & Other Characteristics     \\ \midrule
wz\_td\_vfa\_op       & \gls{wz}     & \gls{td}            & \gls{vfa} & \gls{op}                      \\
wz\_one\_td\_vfa\_op   & \gls{wzo}     & \gls{td}            & \gls{vfa} & \gls{op}                \\
bi\_one\_mc\_tf   & \gls{bi}     & \gls{mc}            & \glspl{ann} & \gls{pg}                \\ \bottomrule
\end{tabular}%
}
\caption{Overview of additional developed policies with \gls{rl}}
\label{tab:rl_others_policies_overview}
\end{table}

\tabref{tab:rl_others_kpis_comp_gain} shows the summarized results. For the detailed results refer to \subsecref{subsec:others_rl_msa_comp_app}.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[!ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllllll@{}}
\toprule
KPI                 & wz\_td\_vfa\_op & wz\_one\_td\_vfa\_op & bi\_one\_mc\_tf\_1l & bi\_one\_mc\_tf\_2l & bi\_one\_mc\_tf\_3l & bi\_one\_mc\_tf\_4l \\ \midrule
Lateness            & $-6.02$                & $24.16$                     & $8.98$                    & $-12.12$                    & $-21.37$                    & $-18.8$                    \\
Wait Time           & $-17.52$                & $77.88$                     & $134.96$                    & $1.88$                    & $-28.06$                    & $-19.39$                    \\
Service Time        & $20.76$                & $13.08$                     & $-7.18$                    & $-15.87$                    & $-18.92$                    & $-18.6$                    \\
Average System Load & $20.58$                & $12.76$                     & $-7.45$                    & $-15.95$                    & $-18.99$                    & $-18.68$                    \\ \bottomrule
\end{tabular}%
}
\caption{Reduction (in \%) across all \glspl{kpi} of the additional policies with \gls{rl} against the \gls{msa} solver}
\label{tab:rl_others_kpis_comp_gain}
\end{table}

\section{Discussion}

\subsection{Optimization}

Using mathematical optimization to solve the assignment problem proves to be an efficient measure, however different solvers yield different solutions and computational complexities. Zeng mentions that \gls{msa} greatly simplifies \gls{dmf} since only those tasks that are immediately available are considered \cite[p. 15]{Zeng2005}. This consideration is done since the \gls{dmf} problem proves to be computationally expensive to solve \cite[p. 13]{Zeng2005},\cite{Garey1990}. Zeng proposes however that by introducing auxiliary variables one can reduce the complexity of \gls{dmf} and effectively solving it \cite[p. 13]{Zeng2005}. This is what has been done in this thesis, as outlined in \secref{sec:opt_policies} by introducing new types of solvers. The \gls{st} ``flagship'' solver significantly outperforms the \gls{msa} solver (refer to \figref{fig:k_batchone_st_opt_kpis_comp}). Having said that, the higher computational complexity of \gls{st} compared to \gls{msa} (see \tabref{tab:big_oh_solvers}) questions the practical use of this solver over the other methods. A $20\%$ gain in respect to lateness requiring a quadratic higher computational complexity poses a dubious trade-off from a business perspective.

\fig[\textwidth]{k_batchone_st_opt_kpis_comp}{\glspl{kpi} comparison between \gls{msa} and \gls{st} under 1-Batch-1}{fig:k_batchone_st_opt_kpis_comp}

Yet another aspect mentioned by Zeng is a more ``social'' aspect: the fairness of a policy \ie how fairly are single users treated by a policy during job assignment \cite[pp. 17-18]{Zeng2005}. \figref{fig:msa_fairness} and \figref{fig:st_fairness} both show how fairly are users treated in the same scenario by the two solvers.

\begin{figure}[!ht]
	\centering
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{img/1_BATCHONE_MSA_NU5_GI3_SIM1000_FAIR}
		\caption{User loads distribution for 1-Batch-1 using \gls{msa}}
		\label{fig:msa_fairness}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{img/1_BATCHONE_ST_NU5_GI3_SIM1000_FAIR}
		\caption{User loads distribution for 1-Batch-1 using \gls{st}}
		\label{fig:st_fairness}
	\end{minipage}
\end{figure}

It is clear that \gls{st} is ``fairer'' at balancing loads across users compared to \gls{msa}. 

\subsection{\glsentrylong{rl}}

Let us first focus on \gls{llqp}, which, as explained in \secref{sec:opt_policies}, focuses on assigning a job to the least loaded qualified person. By comparing the results obtained with \gls{rl} against the optimization method, we clearly see that gains across all \glspl{kpi} are imperceptible (refer to \subsecref{subsec:llqp_rl_msa_comp_app} for more details). This sheds light on two key aspects:
\begin{enumerate*}
	\item \gls{llqp} policies are intrinsically optimized by their nature of implementation.
	\item \gls{rl} methods converge really well (for a $1000$ time steps \gls{llqp} simulation, \gls{llqp} with \gls{td} and \gls{tf} only needs 20 times the simulation time as training in order to perfectly converge to actual \gls{llqp}) and, even if only slightly, exploit internal mechanisms of \gls{llqp} policies to extract better results.
\end{enumerate*}

On the other hand, batch policies exhibit a much bigger optimization potential for which \gls{rl} methods do really adapt well. Lateness improvements in the range of $30\%$ for 1-Batch (see \subsecref{subsec:onebatch_rl_msa_comp_app}) respectively $20\%$ for 1-Batch-1 (see \subsecref{subsec:onebatchone_rl_msa_comp_app}) confirm the previous claim.

Lastly, when accounting for job order during assignment (refer to \subsecref{subsec:batch_size_emulation} for the detailed explanation), improvements are observed only under specific conditions:
\begin{enumerate*}
	\item by using \gls{wzo} (see \figref{fig:wz_one_td_vfa_op}) and 
	\item \glspl{ann} with \gls{1l} (see \figref{fig:bi_one_mc_tf_1l}).
\end{enumerate*}

Having said that, deep \glspl{ann} exhibits worse results compared to their equivalent emulated optimization methods. This can be explained from a twofold perspective:
\begin{enumerate*}
	\item \gls{vgp} and
	\item \gls{egp}.
\end{enumerate*}

In brief, \gls{vgp} states that even very large change in partial derivatives on initial layers have imperceptible effects on subsequent layers \cite{Bengio1994} and \gls{egp} states that huge spikes in the norm of changes in partial derivatives which could potentially grow exponentially can happen under training, thus influencing internal parameters \cite{Bengio1994,Pascanu2012}.

\section{Research Contribution}

\chapter{Conclusion}
\label{ch:conclusion}

\section{Summary}
\section{Resulting Conclusions}
\section{Outlook}

\appendix

\chapter{Optimization Results}

\section{K-Batch}

\subsection{\glsentryshortpl{kpi}}
\label{subsec:kbatch_kpi_app}

\fig[\textwidth]{kbatch_res/3_BATCH_MSA_NU3_GI3_SIM50_KPI}{K-Batch with \gls{msa} \glspl{kpi}}{fig:3_BATCH_MSA_NU3_GI3_SIM50_KPI}
\fig[\textwidth]{kbatch_res/3_BATCH_DMF_NU3_GI3_SIM50_KPI}{K-Batch with \gls{dmf} \glspl{kpi}}{fig:3_BATCH_DMF_NU3_GI3_SIM50_KPI}
\fig[\textwidth]{kbatch_res/3_BATCH_SDMF_NU3_GI3_SIM50_KPI}{K-Batch with \gls{sdmf} \glspl{kpi}}{fig:3_BATCH_SDMF_NU3_GI3_SIM50_KPI}
\fig[\textwidth]{kbatch_res/3_BATCH_ESDMF_NU3_GI3_SIM50_KPI}{K-Batch with \gls{esdmf} \glspl{kpi}}{fig:3_BATCH_ESDMF_NU3_GI3_SIM50_KPI}
\fig[\textwidth]{kbatch_res/3_BATCH_ST_NU3_GI3_SIM50_KPI}{K-Batch with \gls{st} \glspl{kpi}}{fig:3_BATCH_ST_NU3_GI3_SIM50_KPI}

\clearpage

\subsection{Evolution}
\label{subsec:kbatch_evo_app}

\fig[\textwidth]{kbatch_res/3_BATCH_MSA_NU3_GI3_SIM50_EVO}{K-Batch with \gls{msa} evolution}{fig:3_BATCH_MSA_NU3_GI3_SIM50_EVO}
\fig[\textwidth]{kbatch_res/3_BATCH_DMF_NU3_GI3_SIM50_EVO}{K-Batch with \gls{dmf} evolution}{fig:3_BATCH_DMF_NU3_GI3_SIM50_EVO}
\fig[\textwidth]{kbatch_res/3_BATCH_SDMF_NU3_GI3_SIM50_EVO}{K-Batch with \gls{sdmf} evolution}{fig:3_BATCH_SDMF_NU3_GI3_SIM50_EVO}
\fig[\textwidth]{kbatch_res/3_BATCH_ESDMF_NU3_GI3_SIM50_EVO}{K-Batch with \gls{esdmf} evolution}{fig:3_BATCH_ESDMF_NU3_GI3_SIM50_EVO}
\fig[\textwidth]{kbatch_res/3_BATCH_ST_NU3_GI3_SIM50_EVO}{K-Batch with \gls{st} evolution}{fig:3_BATCH_ST_NU3_GI3_SIM50_EVO}

\clearpage

\subsection{Batch Sizes Comparison}
\label{subsec:kbatch_bscomp_app}

\fig[\textwidth]{kbatch_res/1-9_BATCH_MSA_NU3_GI3_SIM50_BSEVAL}{K-Batch with \gls{msa} batch size comparison}{fig:1-9_BATCH_MSA_NU3_GI3_SIM50_BSEVAL}
\fig[\textwidth]{kbatch_res/1-9_BATCH_ST_NU3_GI3_SIM50_BSEVAL}{K-Batch with \gls{st} batch size comparison}{fig:1-9_BATCH_ST_NU3_GI3_SIM50_BSEVAL}

\clearpage

\section{K-Batch-1}

\subsection{\glsentryshortpl{kpi}}
\label{subsec:kbatchone_kpi_app}

\fig[\textwidth]{kbatchone_res/3_BATCHONE_MSA_NU3_GI3_SIM50_KPI}{K-Batch-1 with \gls{msa} \glspl{kpi}}{fig:3_BATCHONE_MSA_NU3_GI3_SIM50_KPI}
\fig[\textwidth]{kbatchone_res/3_BATCHONE_DMF_NU3_GI3_SIM50_KPI}{K-Batch-1 with \gls{dmf} \glspl{kpi}}{fig:3_BATCHONE_DMF_NU3_GI3_SIM50_KPI}
\fig[\textwidth]{kbatchone_res/3_BATCHONE_SDMF_NU3_GI3_SIM50_KPI}{K-Batch-1 with \gls{sdmf} \glspl{kpi}}{fig:3_BATCHONE_SDMF_NU3_GI3_SIM50_KPI}
\fig[\textwidth]{kbatchone_res/3_BATCHONE_ESDMF_NU3_GI3_SIM50_KPI}{K-Batch-1 with \gls{esdmf} \glspl{kpi}}{fig:3_BATCHONE_ESDMF_NU3_GI3_SIM50_KPI}
\fig[\textwidth]{kbatchone_res/3_BATCHONE_ST_NU3_GI3_SIM50_KPI}{K-Batch-1 with \gls{st} \glspl{kpi}}{fig:3_BATCHONE_ST_NU3_GI3_SIM50_KPI}

\clearpage

\subsection{Evolution}
\label{subsec:kbatchone_evo_app}

\fig[\textwidth]{kbatchone_res/3_BATCHONE_MSA_NU3_GI3_SIM50_EVO}{K-Batch-1 with \gls{msa} evolution}{fig:3_BATCHONE_MSA_NU3_GI3_SIM50_EVO}
\fig[\textwidth]{kbatchone_res/3_BATCHONE_DMF_NU3_GI3_SIM50_EVO}{K-Batch-1 with \gls{dmf} evolution}{fig:3_BATCHONE_DMF_NU3_GI3_SIM50_EVO}
\fig[\textwidth]{kbatchone_res/3_BATCHONE_SDMF_NU3_GI3_SIM50_EVO}{K-Batch-1 with \gls{sdmf} evolution}{fig:3_BATCHONE_SDMF_NU3_GI3_SIM50_EVO}
\fig[\textwidth]{kbatchone_res/3_BATCHONE_ESDMF_NU3_GI3_SIM50_EVO}{K-Batch-1 with \gls{esdmf} evolution}{fig:3_BATCHONE_ESDMF_NU3_GI3_SIM50_EVO}
\fig[\textwidth]{kbatchone_res/3_BATCHONE_ST_NU3_GI3_SIM50_EVO}{K-Batch-1 with \gls{st} evolution}{fig:3_BATCHONE_ST_NU3_GI3_SIM50_EVO}

\clearpage

\subsection{Batch Sizes Comparison}
\label{subsec:kbatchone_bscomp_app}

\fig[\textwidth]{kbatchone_res/1-9_BATCHONE_MSA_NU3_GI3_SIM50_BSEVAL}{K-Batch-1 with \gls{msa} batch size comparison}{fig:1-9_BATCHONE_MSA_NU3_GI3_SIM50_BSEVAL}
\fig[\textwidth]{kbatchone_res/1-9_BATCHONE_ST_NU3_GI3_SIM50_BSEVAL}{K-Batch-1 with \gls{st} batch size comparison}{fig:1-9_BATCHONE_ST_NU3_GI3_SIM50_BSEVAL}

\clearpage

\section{\glsentrylong{llqp}}

\subsection{\glsentryshortpl{kpi}}
\label{subsec:llqp_kpi_app}

\fig[\textwidth]{llqp_res/LLQP_NU3_GI3_SIM50_KPI}{\gls{llqp} \glspl{kpi}}{fig:LLQP_NU3_GI3_SIM50_KPI}

\clearpage

\subsection{Evolution}
\label{subsec:llqp_evo_app}

\fig[\textwidth]{llqp_res/LLQP_NU3_GI3_SIM50_EVO}{\gls{llqp} evolution}{fig:LLQP_NU3_GI3_SIM50_EVO}

\clearpage

\section{\glsentrylong{sq}}

\subsection{\glsentryshortpl{kpi}}
\label{subsec:sq_kpi_app}

\fig[\textwidth]{sq_res/SQ_NU3_GI3_SIM50_KPI}{\gls{sq} \glspl{kpi}}{fig:SQ_NU3_GI3_SIM50_KPI}

\clearpage

\subsection{Evolution}
\label{subsec:sq_evo_app}

\fig[\textwidth]{sq_res/SQ_NU3_GI3_SIM50_EVO}{\gls{sq} evolution}{fig:SQ_NU3_GI3_SIM50_EVO}

\clearpage

\chapter{\glsentrylong{rl} Results}

\section{1-Batch}

\subsection{\glsentryshortpl{kpi}}
\label{subsec:onebatch_rl_kpi_app}

\fig[\textwidth]{kbatch_rl_res/1_BATCH_MC_VFA_NU5_GI2_SIM1000_KPI}{1-Batch with \gls{mc} and \gls{vfa} \glspl{kpi}}{fig:1_BATCH_MC_VFA_NU5_GI2_SIM1000_KPI}
\fig[\textwidth]{kbatch_rl_res/1_BATCH_MC_VFA_OP_NU5_GI2_SIM1000_KPI}{1-Batch with \gls{mc}, \gls{vfa} and \gls{op} \glspl{kpi}}{fig:1_BATCH_MC_VFA_OP_NU5_GI2_SIM1000_KPI}
\fig[\textwidth]{kbatch_rl_res/1_BATCH_MC_VFA_OPEP_NU5_GI2_SIM1000_KPI}{1-Batch with \gls{mc}, \gls{vfa}, \gls{op} and \gls{ep} \glspl{kpi}}{fig:1_BATCH_MC_VFA_OPEP_NU5_GI2_SIM1000_KPI}
\fig[\textwidth]{kbatch_rl_res/1_BATCH_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_KPI}{1-Batch with \gls{td}, \gls{vfa} and \gls{op} \glspl{kpi}}{fig:1_BATCH_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_KPI}

\clearpage

\subsection{Evolution}
\label{subsec:onebatch_rl_evo_app}

%\fig[\textwidth]{kbatch_rl_res/1_BATCH_MC_VFA_NU5_GI2_SIM1000_EVO}{1-Batch with \gls{mc} and \gls{vfa} evolution}{fig:1_BATCH_MC_VFA_NU5_GI2_SIM1000_EVO}
%\fig[\textwidth]{kbatch_rl_res/1_BATCH_MC_VFA_OP_NU5_GI2_SIM1000_EVO}{1-Batch with \gls{mc}, \gls{vfa} and \gls{op} evolution}{fig:1_BATCH_MC_VFA_OP_NU5_GI2_SIM1000_EVO}
%\fig[\textwidth]{kbatch_rl_res/1_BATCH_MC_VFA_OPEP_NU5_GI2_SIM1000_EVO}{1-Batch with \gls{mc}, \gls{vfa}, \gls{op} and \gls{ep} evolution}{fig:1_BATCH_MC_VFA_OPEP_NU5_GI2_SIM1000_EVO}
%\fig[\textwidth]{kbatch_rl_res/1_BATCH_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_EVO}{1-Batch with \gls{td}, \gls{vfa} and \gls{op} evolution}{fig:1_BATCH_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_EVO}

\clearpage

\subsection{Comparison with \glsentryshort{msa}}
\label{subsec:onebatch_rl_msa_comp_app}

\fig[\textwidth]{kbatch_rl_res/k_batch_mc_vfa}{1-Batch with \gls{mc} and \gls{vfa} \gls{msa} comparison}{fig:k_batch_mc_vfa}
\fig[\textwidth]{kbatch_rl_res/k_batch_mc_vfa_op}{1-Batch with \gls{mc}, \gls{vfa} and \gls{op} \gls{msa} comparison}{fig:k_batch_mc_vfa_op}
\fig[\textwidth]{kbatch_rl_res/k_batch_mc_vfa_opep}{1-Batch with \gls{mc}, \gls{vfa}, \gls{op} and \gls{ep} \gls{msa} comparison}{fig:k_batch_mc_vfa_opep}
\fig[\textwidth]{kbatch_rl_res/k_batch_td_vfa_op}{1-Batch with \gls{td}, \gls{vfa}, \gls{op} and \gls{ep} \gls{msa} comparison}{fig:k_batch_td_vfa_op}

\section{1-Batch-1}

\subsection{\glsentryshortpl{kpi}}
\label{subsec:onebatchone_rl_kpi_app}

\fig[\textwidth]{kbatchone_rl_res/1_BATCHONE_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_KPI}{1-Batch-1 with \gls{td}, \gls{vfa} and \gls{op} \glspl{kpi}}{fig:1_BATCHONE_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_KPI}

\clearpage

\subsection{Evolution}
\label{subsec:onebatchone_rl_evo_app}

%\fig[\textwidth]{kbatchone_rl_res/1_BATCHONE_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_EVO}{1-Batch-1 with \gls{td}, \gls{vfa} and \gls{op} evolution}{fig:1_BATCHONE_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_EVO}

\clearpage

\subsection{Comparison with \glsentryshort{msa}}
\label{subsec:onebatchone_rl_msa_comp_app}

\fig[\textwidth]{kbatchone_rl_res/k_batchone_td_vfa_op}{1-Batch-1 with \gls{td}, \gls{vfa} and \gls{op} \gls{msa} comparison}{fig:k_batchone_td_vfa_op}

\clearpage

\section{\glsentrylong{llqp}}

\subsection{\glsentryshortpl{kpi}}
\label{subsec:llqp_rl_kpi_app}

\fig[\textwidth]{llqp_rl_res/LLQP_MC_VFA_OP_NU5_GI2_SIM1000_KPI}{\gls{llqp} with \gls{mc}, \gls{vfa} and \gls{op} \glspl{kpi}}{fig:LLQP_MC_VFA_OP_NU5_GI2_SIM1000_KPI}
\fig[\textwidth]{llqp_rl_res/LLQP_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_KPI}{\gls{llqp} with \gls{td}, \gls{vfa} and \gls{op} \glspl{kpi}}{fig:LLQP_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_KPI}
\fig[\textwidth]{llqp_rl_res/LLQP_TD_TF_OP_NU5_GI2_TRSD2_SIM1000_KPI}{\gls{llqp} with \gls{td}, \gls{tf} and \gls{op} \glspl{kpi}}{fig:LLQP_TD_TF_OP_NU5_GI2_TRSD2_SIM1000_KPI}

\clearpage

\subsection{Evolution}
\label{subsec:llqp_rl_evo_app}

%\fig[\textwidth]{llqp_rl_res/LLQP_MC_VFA_OP_NU5_GI2_SIM1000_EVO}{\gls{llqp} with \gls{mc}, \gls{vfa} and \gls{op} evolution}{fig:LLQP_MC_VFA_OP_NU5_GI2_SIM1000_EVO}
%\fig[\textwidth]{llqp_rl_res/LLQP_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_EVO}{\gls{llqp} with \gls{td}, \gls{vfa} and \gls{op} evolution}{fig:LLQP_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_EVO}
%\fig[\textwidth]{llqp_rl_res/LLQP_TD_TF_OP_NU5_GI2_TRSD2_SIM1000_EVO}{\gls{llqp} with \gls{td}, \gls{tf} and \gls{op} evolution}{fig:LLQP_TD_TF_OP_NU5_GI2_TRSD2_SIM1000_EVO}

\clearpage

\subsection{Comparison with \glsentryshort{msa}}
\label{subsec:llqp_rl_msa_comp_app}

\fig[\textwidth]{llqp_rl_res/llqp_mc_vfa_op}{\gls{llqp} with \gls{mc}, \gls{vfa} and \gls{op} \gls{msa} comparison}{fig:llqp_mc_vfa_op}
\fig[\textwidth]{llqp_rl_res/llqp_td_vfa_op}{\gls{llqp} with \gls{td}, \gls{vfa} and \gls{op} \gls{msa} comparison}{fig:llqp_td_vfa_op}
\fig[\textwidth]{llqp_rl_res/llqp_td_tf_op}{\gls{llqp} with \gls{td}, \gls{tf} and \gls{op} \gls{msa} comparison}{fig:llqp_td_tf_op}

\clearpage

\subsection{Additional \glsentrylong{llqp} Policies}
\label{subsec:llqp_rl_more_policies_app}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
% \usepackage{graphicx}
\begin{table}[!ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lllll@{}}
\toprule
Technical Name          & Policy Type & Update Method & $Q$ Value Method               & Other Characteristics     \\ \midrule
llqp\_mc\_vfa       & \gls{llqp}     & \gls{mc}            & \gls{vfa} & None                      \\
llqp\_td\_vfa   & \gls{llqp}     & \gls{td}            & \gls{vfa} & None                \\
llqp\_mc\_pg   & \gls{llqp}     & \gls{mc}            & \gls{pg} & None                \\
llqp\_mc\_pg\_wb   & \gls{llqp}     & \gls{mc}            & \gls{pg} & With Baseline                \\
llqp\_td\_pg\_ac   & \gls{llqp}     & \gls{td}            & \gls{pg} & Actor Critic                \\
llqp\_td\_pg\_avac   & \gls{llqp}     & \gls{td}            & \gls{pg} & Action Value Actor Critic                \\ \bottomrule
\end{tabular}%
}
\caption{Overview of additional \gls{llqp} policies with \gls{rl}}
\label{tab:rl_llqp_add_policies_overview}
\end{table}

\clearpage

\section{Others}

\subsection{\glsentryshortpl{kpi}}
\label{subsec:others_rl_kpi_app}

\fig[\textwidth]{others_rl_res/5_WZ_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_KPI}{\gls{wz} with \gls{td}, \gls{vfa} and \gls{op} \glspl{kpi}}{fig:5_WZ_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_KPI}
\fig[\textwidth]{others_rl_res/1_WZ_ONE_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_KPI}{\gls{wzo} with \gls{td}, \gls{vfa} and \gls{op} \glspl{kpi}}{fig:1_WZ_ONE_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_KPI}
\fig[\textwidth]{others_rl_res/1_BI_ONE_MC_TF_1L_NU5_GI2_SIM1000_KPI}{\gls{bi} with \gls{mc}, \gls{tf} and \gls{1l} \glspl{kpi}}{fig:1_BI_ONE_MC_TF_1L_NU5_GI2_SIM1000_KPI}
\fig[\textwidth]{others_rl_res/1_BI_ONE_MC_TF_2L_NU5_GI2_SIM1000_KPI}{\gls{bi} with \gls{mc}, \gls{tf} and \gls{2l} \glspl{kpi}}{fig:1_BI_ONE_MC_TF_2L_NU5_GI2_SIM1000_KPI}
\fig[\textwidth]{others_rl_res/1_BI_ONE_MC_TF_3L_NU5_GI2_SIM1000_KPI}{\gls{bi} with \gls{mc}, \gls{tf} and \gls{3l} \glspl{kpi}}{fig:1_BI_ONE_MC_TF_3L_NU5_GI2_SIM1000_KPI}
\fig[\textwidth]{others_rl_res/1_BI_ONE_MC_TF_4L_NU5_GI2_SIM1000_KPI}{\gls{bi} with \gls{mc}, \gls{tf} and \gls{4l} \glspl{kpi}}{fig:1_BI_ONE_MC_TF_4L_NU5_GI2_SIM1000_KPI}

\clearpage

\subsection{Evolution}
\label{subsec:others_rl_evo_app}

%\fig[\textwidth]{others_rl_res/5_WZ_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_EVO}{\gls{wz} with \gls{td}, \gls{vfa} and \gls{op} evolution}{fig:5_WZ_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_EVO}
%\fig[\textwidth]{others_rl_res/1_WZ_ONE_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_EVO}{\gls{wzo} with \gls{td}, \gls{vfa} and \gls{op} evolution}{fig:1_WZ_ONE_TD_VFA_OP_NU5_GI2_TRSD2_SIM1000_EVO}
%\fig[\textwidth]{others_rl_res/1_BI_ONE_MC_TF_1L_NU5_GI2_SIM1000_EVO}{\gls{bi} with \gls{mc}, \gls{tf} and \gls{1l} evolution}{fig:1_BI_ONE_MC_TF_1L_NU5_GI2_SIM1000_EVO}
%\fig[\textwidth]{others_rl_res/1_BI_ONE_MC_TF_2L_NU5_GI2_SIM1000_EVO}{\gls{bi} with \gls{mc}, \gls{tf} and \gls{2l} evolution}{fig:1_BI_ONE_MC_TF_2L_NU5_GI2_SIM1000_EVO}
%\fig[\textwidth]{others_rl_res/1_BI_ONE_MC_TF_3L_NU5_GI2_SIM1000_EVO}{\gls{bi} with \gls{mc}, \gls{tf} and \gls{3l} evolution}{fig:1_BI_ONE_MC_TF_3L_NU5_GI2_SIM1000_EVO}
%\fig[\textwidth]{others_rl_res/1_BI_ONE_MC_TF_4L_NU5_GI2_SIM1000_EVO}{\gls{bi} with \gls{mc}, \gls{tf} and \gls{4l} evolution}{fig:1_BI_ONE_MC_TF_4L_NU5_GI2_SIM1000_EVO}

\clearpage

\subsection{Comparison with \glsentryshort{msa}}
\label{subsec:others_rl_msa_comp_app}

\fig[\textwidth]{others_rl_res/wz_td_vfa_op}{\gls{wz} with \gls{td}, \gls{vfa} and \gls{op} \glspl{msa} comparison}{fig:wz_td_vfa_op}
\fig[\textwidth]{others_rl_res/wz_one_td_vfa_op}{\gls{wzo} with \gls{td}, \gls{vfa} and \gls{op} \glspl{msa} comparison}{fig:wz_one_td_vfa_op}
\fig[\textwidth]{others_rl_res/bi_one_mc_tf_1l}{\gls{bi} with \gls{mc}, \gls{tf} and \gls{1l} \glspl{msa} comparison}{fig:bi_one_mc_tf_1l}
\fig[\textwidth]{others_rl_res/bi_one_mc_tf_2l}{\gls{bi} with \gls{mc}, \gls{tf} and \gls{2l} \glspl{msa} comparison}{fig:bi_one_mc_tf_2l}
\fig[\textwidth]{others_rl_res/bi_one_mc_tf_3l}{\gls{bi} with \gls{mc}, \gls{tf} and \gls{3l} \glspl{msa} comparison}{fig:bi_one_mc_tf_3l}
\fig[\textwidth]{others_rl_res/bi_one_mc_tf_4l}{\gls{bi} with \gls{mc}, \gls{tf} and \gls{4l} \glspl{msa} comparison}{fig:bi_one_mc_tf_4l}

\clearpage

\backmatter

\printglossaries

\nocite{*}

\bibliographystyle{apalike}
\bibliography{sources}

\end{document}
