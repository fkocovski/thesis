\documentclass{seal_thesis}

\usepackage{url}
\usepackage[inline]{enumitem}
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage[colorlinks=true]{hyperref}
% used to fix structure
\usepackage{bookmark}
\usepackage{amsfonts}


\thesisType{Master Thesis}
\date{\today}
\title{BPM}
\subtitle{Discrete Event Simulation for Optimal Role Resolution in Workflow Processes}
\author{Filip Ko\v{c}ovski}
\home{Lugano} % Geburtsort
\country{Switzerland}
\legi{10-932-994}
\prof{Prof. Dr. Daning Hu}
\assistent{Dr. Markus Uhr}
\email{filip.kocovski@uzh.ch}
\begindate{November 15, 2016}
\enddate{May 15, 2017}

\begin{document}
\maketitle

\frontmatter

\begin{acknowledgements}

\end{acknowledgements}

\begin{abstract}

\end{abstract}

\begin{zusammenfassung}

\end{zusammenfassung}

\tableofcontents
\listoffigures
\listoftables
\lstlistoflistings

\pagebreak

\mainmatter

Structure for the thesis adapted from \url{https://wwz.unibas.ch/fileadmin/wwz/redaktion/fmgt/Images/FinanzmanagementLeitfadenfuerArbeiten.pdf}

\chapter{Introduction}
\label{ch:intro}

\section{Problem Definition}

Workflows are IT solutions that can help increase efficiency and get tasks done better and faster. However a key element of each workflow process still remains the human aspect. This human aspect can take many facets, such as humans analyzing a process, humans designing a process and humans executing the latter. This thesis focuses on the latter, \ie where human agents interact with the workflow process in order to work on tasks. A business process that has been efficiently analyzed and subsequently optimally implemented still cannot ensure optimal execution, or no optimal execution can be achieved while a human intervention for task execution is present. It is here that optimal role resolution comes in play: optimally choosing and assigning a specific task inside the workflow process to the best possible actor is a non trivial task that has to be solved in order to close the ``optimization'' circle that workflow engines advertise.

This field is relevant since an optimal role resolution can bring optimization from many sides:
\begin{enumerate*}
	\item cost savings,
	\item fairness in workload assignment
	\item optimal resources usage.
\end{enumerate*}

Currently many different workflow engines exist, ranging from complete fully functional suites and down to extensible frameworks that allow the implementer to adapt it to its own needs. However all these solutions lack optimality in the task assignment sector.

\section{Objectives}
\label{sec:objectives}

The objectives of these thesis build upon the work of Zeng and Zhao \cite{Zeng2005}, in which they depicted preliminary policies for optimal role resolution, and extends these capabilities from a threefold perspective:
\begin{enumerate*}
	\item further develops the mathematical premises and extends the capabilities of the batching policies proposed by Zeng and Zhao
	\item explores the capabilities offered by reinforcement learning as addition and improvement for even precises, faster and better task assignment
	\item deployment of the aforementioned optimization techniques in an operative environment of a real estate company using a workflow engine.
\end{enumerate*}

Formally, this thesis tries to answer the following research questions:

\begin{enumerate}
	\item Are there better optimization techniques for optimal role resolution techniques inside workflow processes?
	\item Is the deployment of optimization policies in a working environment for a workflow engine a critical success factor?
	\item How is optimization in the field of task assignment perceived by the workflow users (actors)?
\end{enumerate}

\section{Thesis Structure}

This thesis is subdivided in five main chapters:

\begin{itemize}
	\item \chpref{ch:intro} gives an overview of why the chosen topic is relevant, what is the current context of the work and how this work fits in. It moreover articulates the central research questions that permeate this thesis and gives an overview of this essay
	\item \chpref{ch:foundations} gives an overview of the most important conceptual definitions and the state of the art literature review in the touched thematic topics of this work. Conclusively this chapter critically reflects upon the existing literature and exposes the deficits that this thesis aims filling
	\item \chpref{ch:methodology} gives an overview of the approach used for the research, \eg the analysis environment and the used tools, states the hypothesis that wants to be proved and eventually describes statistically and qualitatively the data sets upon which the methodology is applied
	\item \chpref{ch:empirical_analysis} builds upon \chpref{ch:methodology} and makes its way into the hypothesis test field and the respective analysis results. Furthermore looks introspectively on the data correlation and gives an interpretation of the latter. Eventually in this section a statement about the contribution that the results bring into this field is given
	\item \chpref{ch:conclusion} is the culminating chapter in which a summary of the key findings of the thesis are outlined, the research questions posed in \secref{sec:objectives} are answered by looking at the actual usability, limitations and to whom the results are most applicable. Finally outlooks about the future trends and how the empirical results of this thesis can be extended by prospective researchers.
\end{itemize}

\chapter{Theoretical Foundations}
\label{ch:foundations}

\section{Definitions}
\subsection{Queueing Theory}
\subsection{Workflow Processes}
\subsection{Reinforcement Learning}
\subsection{Mixed Integer Linear Optimization}
\subsection{Discrete Event Simulation}
\section{Literature Overview}
\label{sec:literature_overview}

This section servers as an overview of the state of the art literature that exists and has been used as a foundation basis for this work. \secref{sec:literature_overview} is divided in different thematic subsections.

\subsection{Queueing}

Queuing is a topic that talks about how people or more general agents are to be server while waiting.

Starting with one of the most notable contributions to this field done by Kendall in 1953 and his work on the Markov chains in queuing theory, where he formally defines different types of queues \cite{Kendall1953}.

In 2002, Adan describes the necessary basic concepts for queuing theory and an important topic here is the statistical foundation outlined in his work about different modeling techniques for randomized generation rates, such as the Erlang's distributions \cite{Adan2002}.

Pinedo outlines in his work in 2008 the most prominent key metrics that can be used in order to assert and measure queues performance \cite{Pinedo2008}.

Sun and Zhao in their work cover the aspect of formal analysis for workflow models and they claim that it should help ``...alleviating the intellectual challenge faced by business analysts when creating workflow models'' \cite{Sun2013}.

\subsection{Workflow}
\label{subsec:workflow}

A good starting point in the workflow thematic is Macintosh's work in which he gives an overview of the five levels of process maturity \cite{Macintosh1993}:

\begin{enumerate}
	\item Initial, the process has to be set up
	\item Repeatable, the process has to be repeatable
	\item Defined, documentation standardization of processes
	\item Managed, measurement and control of processes
	\item Optimizing, continuous process improvement
\end{enumerate}

Even though Georgakopoulos' work dates back to 1995, he still gives a comprehensive business oriented overview of the different workflow technologies present on the market \cite{Georgakopoulos1995}.

On this note, Giaglis lays out four different process perspectives:
\begin{enumerate*}
	\item Functional
	\item Behavioral
	\item Organizational
	\item Informational
\end{enumerate*}

His framework focuses on three dimensions:
\begin{enumerate*}
	\item Breadth, where modeling goals are typically addressed by technique
	\item Depth, where modeling perspectives are covered
	\item Fit, where typical project to which techniques can be fit
\end{enumerate*}

The presented framework is used to combine the three different dimensions in order to assert a possible best fit of a specific modeling technique based on which approach to be used under the constraints of a modeling perspective to cover \cite{Giaglis2001}.

Mentzas focuses on a qualitative level on how workflow technologies can facilitate implementation of business processes by focusing on the pros and cons of adopting alternative workflow modeling techniques \cite{Mentzas2001}. Moreover he formally defines what a workflow management system is and subdivides it in three main categories:
\begin{enumerate*}
	\item Process modeling
	\item Process re-engineering
	\item Workflow implementation and automation
\end{enumerate*}

Each level of maturity as defined by Macintosh requires a different model, such as the first three levels might require more descriptive models whereas levels four and five require decision support keen models in order to monitor and control processes \cite{Mentzas2001}.

Aguilar describes the main modeling techniques existing with workflow being one of them \cite{Aguilar-Saven2004}.

The key core topics on which this thesis lays its foundations upon is the work done by Zeng in 2005. Effective role resolution, \ie the mechanism of assigning tasks to individual workers at runtime according to the role qualification defined in the workflow model \cite{Zeng2005}, is the core aspect that is being extended during this thesis work.

Zeng differentiates between staffing decisions and role resolution, with the former being the assignment one or more role to each user and the latter being the assignment of a specific task to an appropriate worker at runtime \cite{Zeng2005}. Staffing decisions are usually made off-line and periodically, thus being more of a strategic nature \cite{Zeng2005}. If role resolution were to be made on-line it could translate to a major operational level decision, \ie the differentiation between strategic vs. operational playing role \cite{Zeng2005}.

He moreover defines three roles a workflow can fulfill:
\begin{enumerate*}
	\item System built-in policies
	\item User customizable policies
	\item Rule based policies
\end{enumerate*}

Considering capacities of resources restrictions under the assignment problem is an NP-hard computational problem and in his work Zeng focuses on how to solve the assignment problem and scheduling decisions with consideration of worker's preferences \cite{Zeng2005}. For this purpose he defines five workflow resolution policies:

\begin{enumerate}
	\item Load balanced approach (LLQP)
	\item Shared queue (SQ)
	\item K-Batch
	\item K-Batch-1
	\item 1-Batch-1
\end{enumerate}

For all batching policies a simplified version of the dynamic minimization of the maximum flowtime (DMF) has to be solved \cite{Zeng2005}.

Zeng's key findings are outlined as follows:
\begin{enumerate*}
	\item Batching policies to be used when system load is medium to high
	\item Processing time variation has major impact on system performance, \ie higher variation favors optimization based policies
	\item Average workload and workload variation can be significantly reduced by online optimization
	\item 1-Batch-1 online optimization policy yields best results in operational conditions
\end{enumerate*}

Interestingly enough, workflow implementation in real world cases is not always only coupled with directly measurable effects, sometimes even unexpected results happen. What is called the ``workflow paradox'' according to Reijers is the fact that the very fact of companies accepting requests for workflow introduction might actually be the most promising way that leads to potentially better and more suitable alternatives \cite{Reijers2005}.

Specifically speaking on the data flow inside workflow processes, one has to consider possible anomalies that might happens. This has been extensively studies by Sun \etal where they formally define data flow methodologies for detecting such anomalies \cite{Sun2006}. Their framework is divided in two components:
\begin{enumerate*}
	\item Data flow specification
	\item Data flow analysis
\end{enumerate*}

Yet again we stumble upon mentioning that simulation for workflow management systems is usually inefficient and inaccurate \cite{Sun2006}. They moreover discuss aspects that data requirements have been analyzed but the required methodologies on discovering data flow errors have not been extensively researched \cite{Sun2006}.

A more recent taxonomy of different BPM application is given by a collaboration between SAP and accenture in 2009 \cite{EvolvedTechnologist2009}.

In the realm of workflow processes and engines BPMN's notation permeates the field and the work of Silver summarizes these foundations very well \cite{Silver2009}.

An analysis of the critical success factors (CSF) for BPM is required in order to assert a product validity and this has been done by Trkman where he defines CSF from three perspectives \cite{Trkman2010}:
\begin{enumerate*}
	\item Contingency theory
	\item Dynamic capabilities
	\item Task-technology fit theory
\end{enumerate*}


Change management in workflow is yet another interesting aspect that should be considered and this has been broadly studied by Wang where he developed an analytical framework for workflow change management through formal modeling of workflow constraints \cite{Wang2011}.

In companies different types of workflow models can exist and Fan focuses on two of these, namely:
\begin{enumerate*}
	\item Conceptual
	\item Logical
\end{enumerate*}

Conceptual models serve as documentation for generic process requirements whereas logical models are used as definitions for technology oriented requirements \cite{Fan2012}. One difficult aspect is the transition from the former to the latter and Fan proposes a formal approach to efficiently support such transitions \cite{Fan2012}.

\subsection{Reinforcement Learning}

Reinforcement learning is a branch of machine learning that promises to overcome the drawbacks posed by the latter by not requiring a training set for efficient machine decisions.

Policy gradient methods with value function approximation and their convergence is of vital importance and this can be achieved by representing the policy by an own function approximation which is independent of the value function and it is updated according to gradient of the expected rewards with respect to the afore mentioned policy \cite{Sutton1999}.

Discretization of the state action space is not always feasible and different techniques have to be used for tractability. Smith proposes such an approach which he calls ``self-organizing map'' \cite{Smith2002}.

In their work, Schaul \etal have developed a modular machine learning library for python that contains different algorithm implementations such as Q-learning, SARSA and REINFORCE and yet also natural actor-critic and neural-fitted implementations such as Q-iteration, recurrent policy gradients, state-dependent exploration and reward-weighted regressions \cite{Schaul2010}.

As Markov Decision Processes grow in size, so does the required computational memory to solve possible discrete lookup tables modeling the state-actions spaces that characterizes them. Notable examples that show how large some of the most common problems can be:
\begin{enumerate*}
	\item the game of backgammon has a total of $10^{20}$ states
	\item the traditional Chinese abstract board game Go has an estimated total of $10^{170}$ states
	\item flying a helicopter or having a robot move in space all require a continuous state space.
\end{enumerate*}

This very large state space requirement is a clear limitation to lookup tables. Even if memory would not be a constraint, the actual learning from such tables would be infeasible. In order to pragmatically learn by reinforcement on such huge problems, value function approximation in the domain of reinforcement learning proves to be a viable solution. For different types of reinforcement learning approaches, \ie Monte-Carlo (MC) or Temporal Difference (TD) methods exist different types of value function approximation, ranging from simple linear combinations of features for MC to neural networks for TD learning. All these different methodologies are outlined in the tutorial by Geramifard \cite{Geramifard2013}.

When working with on-line algorithms such as TD(0) it is important to choose correct parameters for an effective learning process, otherwise the learning algorithm put in place might never converge towards an optimal solution. This aspect is being discussed by Korda in which he depicts different non-asymptotic bounds for the temporal difference learning algorithms \cite{Korda2014}.

There are two main fields in reinforcement learning, one is using value function approximation for either the state value function or for using control mechanisms with the state action value function, while the other one is using policy gradient methods for policy optimization. The latter offers different methods such as the naive finite difference methods, monte carlo based policy gradient methods and finally actor critic policy gradient methods \cite{Silver2014}.

Notable works in the field of reinforcement learning and its application include Google DeepMind work on novel algorithms for tackling fields previously barely scratched, as mentioned by Mnih \etal and Silver \etal \cite{Mnih2015,Silver2016}.

Sutton started working on the reinforcement learning topic in the early nineties and is now planning his third edition of his famous book on machine learning, which is due in 2017 \cite{Sutton1998}. In our case reinforcement learning is used in order for the policies to be able to alone get better by continuously analyzing their own decision models and optimize upon them.

\subsection{Optimization}

For all batching policies implemented in this work, a mixed integer optimization was solved in order to optimally assign jobs to users in the workflow processes. The generalized assignment problem is a very well known problem in combinatorial mathematics. Cattrysse gives an overview of different algorithms for solving the generalized assignment problem \cite{Cattrysse1992}. Heuristics are also a viable solution for solving such adaptation of the generalized assignment problem, as Racer states \cite{Racer1994}. Moreover a global perspective of optimization from a mathematical perspective is given in Boyd's work on convex optimization \cite{Boyd2004}.

Last but not least, according to the AIMMS guidelines, there are different linear programming tricks that can be used to shape such problems in solvable outlines \cite{Bisschop2016}. In this thesis, a specific linear programming trick, called either-or constraints, was used by adding so called auxiliary variables to the evaluation method presented in order to efficiently solve an otherwise non solvable equation \cite[p. 77]{Bisschop2016}.

\subsection{Simulation}

Simulating queues can prove to be extremely difficult. The main differentiation needed here is that between continuous and step functions: the former is the result when the events being simulated yield values that if plotted against the simulation time give a continuous function. On the other hand, if we simulate events that yield discrete values, such as inventory changes in a storage facility and plot the results against the simulation time we would get so called step functions \cite{Matloff2008}.

According to Matloff, there exist different world views for discrete event programing, as he calls them paradigms \cite{Matloff2008}:

\begin{enumerate}
	\item Activity oriented
	\item Event oriented
	\item Process oriented
\end{enumerate}

Activity oriented can be summarized as simulation events where time is being subdivided in tiny intervals at which the program checks the status for all simulated entities. Since very small subdivisions of time are possible in such types of simulations, it is clear that the program might prove extremely inefficient, since most of the time there won't be any change in state for the simulated entities \cite{Matloff2008}. Event oriented circumnavigate this issue by advancing the simulation time directly to the next event to be simulated. By filling these gaps, a dramatical increase in computation can be observed \cite{Matloff2008}. Last but not least, the process oriented simulation models each simulation activity as a process or thread. Management of threads has steadily decreased in todays computation since many different packages for governing such tasks.

On another note, Bahouth focuses in work on algorithmic analysis of discrete event simulation supplemented with focus on factors such as compiler efficiency, code interpretation and caching memory issues \cite{Bahouth2007}. According to his findings, a significant speedup can be achieved if one addresses the afore mentioned facets.

\section{Research Deficit}

\chapter{Methodology}
\label{ch:methodology}

\section{Analysis Structure}
\subsection{Tools}
Different tools were used in the analysis environment in order to efficiently simulate and analyze the work of this thesis.

The whole architecture is subdivided as follows:
\begin{enumerate}
	\item The simulation environment is based on \texttt{Python 3.5.2}\foot{https://www.python.org}{06.01.2017} and as a discrete event simulation the \texttt{SimPy}\foot{https://simpy.readthedocs.io/en/latest/}{06.01.2017} package is used.
	\item The resulting data are interpreted and analyzed using \texttt{R}\foot{https://www.r-project.org}{06.01.2017}.
	\item The workflow engine itself is \texttt{Java}\foot{https://www.java.com/en/}{06.01.2017} based and uses the \texttt{jBPM}\foot{https://www.jbpm.org}{06.01.2017} suite.
	\item \texttt{PyBrain}\foot{http://pybrain.org}{04.01.2017} is the library used for reinforcement learning.
	\item Coding IDE used were \texttt{PyCharm 2016.3}\foot{https://www.jetbrains.com/pycharm/}{06.01.2017} for \texttt{Python} respectively \texttt{IntelliJ IDEA 2016.3} for \texttt{Java}\foot{https://www.jetbrains.com/idea/}{06.01.2017}.
	\item For solving the mixed integer problems for batching policies \texttt{Gurobi 7.0.1}\foot{http://www.gurobi.com}{06.01.2017} was used.
	\item In order to allow a seamless integration between the optimal resolution policies implemented in \texttt{Python} and the workflow engine developed in \texttt{Java}, \texttt{Jython 2.7.0}\foot{http://www.jython.org}{06.01.2017} was used.
\end{enumerate}

\subsection{Discrete event simulation using SimPy}

SimPy is a Python process-based discrete-event simulation framework. It exploits Python generators according to which it models its processes.

Active components such as agents in a workflow are modeled as processes which live inside an environment and the interaction between them happens via events.

As previously mentioned, processes in SimPy are described by Python generators. During their lifetime they create events yield (Note that with the term \texttt{yield} here it is to be understood as Python's yield statements)\foot{https://docs.python.org/3.5/reference/simple_stmts.html\#the-yield-statement}{06.01.2017} them to the environment, which then wait until they are triggered. The important logic to understand here is how SimPy treats yielded events: when a process yields an event it gets suspended. From the suspended state a process gets resumed when the event actually occurs (or in SimPy's notation when it gets triggered).

SimPy offers a built-in event type called \texttt{Timeout}: events of this type are automatically triggered after a determined simulation time step. Consistency is asserted since a timeout event are created and called by called the appropriate method of the passed \texttt{Environment}.

\subsection{Analysis Environment}

The analysis environment consists in an object-oriented implementations of workflow process elements such as user task, starting, decision and end nodes which have been developed to allow the simulation framework to effectively run. This object-oriented exoskeleton implementation of the workflow elements can be seen depicted in \figref{fig:workflow_elements}.

\fig[\textwidth]{workflow_elements}{Workflow elements}{fig:workflow_elements}

An implicit object that is not part of the workflow elements is the token: a token is to be understood as an object that travels through the whole process and all its elements and gets worked by in different ways by them. In this implementation the token object is directly a policy element as it can be seen in \figref{fig:policies_init}.

\fig[0.5\textwidth]{policies_init}{Policies initialization objects}{fig:policies_init}

The core elements of a workflow process (relevant for the simulation environment) are start nodes, user tasks, decision nodes and end nodes. Start events are used to indicate where and how a process starts and usually each process has only one such event \cite[p. 42]{Silver2009}. No distinction between trigger types is being made.

\subsubsection{Start event}

Start event objects require a simulation environment, a policy to be used, a generation interval and the number of tokens to be generated. The simulation environment is generated with SimPy, the policy object is initialized prior to the simulation and the tokens to be generated is a plain scalar value. The generation interval is generated in a three step process:
\begin{enumerate*}
 	\item before the simulation starts, a fixed service interval time unit $s$, number of users $n$ and an average system load $l$ are set. Analog to Zeng's and Zhao's work, the generation $\lambda$ interval follows a Poisson distribution \cite{Zeng2005} and is defined in \equref{eq:generation_interval}
 	\item for a Poisson random exponential sampling of the generation rate, \texttt{NumPy}'s implementation of its exponential distribution is used\foot{https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.exponential.html}{06.01.2017}. The scale parameter $\beta$, which is the inverse of the rate parameter $\lambda = 1/\beta$ is used, meaning that the generation rate $\lambda$ defined in \equref{eq:generation_interval} is passed to the start event inverted.
 \end{enumerate*}

\begin{equation}
\label{eq:generation_interval}
	\lambda = \frac{l n}{s}
\end{equation}

\lstref{lst:generate_tokens} shows the token generation method for the start event. As shown, this method generates infinitely many tokens and for each token a random exponential simulation time is being drawn. Note that a random state with a fixed seed is used in order to preserve generality across multiple simulation runs. Furthermore the aforementioned timeout event of the SimPy framework can be seen in action. For each token a timeout event is being automatically triggered after its sampled arrival time has elapsed.

\begin{lstlisting}[caption=Token generation method for start event object,label=lst:generate_tokens,language=Python]
    def generate_tokens(self):
        while True:
            exp_arrival = round(RANDOM_STATE.exponential(self.generation_interval), 1)
            yield self.env.timeout(exp_arrival)
            token = Token()
\end{lstlisting}

Even though tokens are generated infinitely, this process is controlled from the simulation environment where a discrete simulation time steps have to be set, as it can be seen from \lstref{lst:simulation_steps}.

This can be interpreted as that the whole simulation will persist for 100 time steps and it will then stop when the internal clock reaches 100. Please note that events that have been scheduled for time step 100 will not be processed. The logic is similar to a new environment where the clock is zero and no event have been processed yet.

\begin{lstlisting}[caption=Starting the simulation with discrete time steps,label=lst:simulation_steps,language=Python]
    # "global" variables
    SIM_TIME = 100
    ...
    # runs simulation
    env.run(until=SIM_TIME)
\end{lstlisting}

\subsubsection{User task}

User task objects also require a simulation environment, a policy, a descriptive name, a knowledge level, a service interval and task variability. Each user task has a unique \texttt{child} field which is being set prior to starting the simulation by the method depicted in \lstref{lst:connect}.

\begin{lstlisting}[caption=Method used to connect workflow elements,label=lst:connect,language=Python]
    def connect(source, destination):
    if isinstance(source, ExclusiveGatewayDivergent):
        for child in destination:
            source.children.append(child)
    elif isinstance(source, (StartEvent, UserTask, EndEvent, ExclusiveGatewayConvergent)):
        source.child = destination
\end{lstlisting}

In regards to parameters service interval and task variability a detailed explanation is required. Both are used to randomly sample service rate intervals for each user active during the simulation. Zeng and Zhao in their work follow a two way process to generate such intervals \cite[p. 8]{Zeng2005}. However in this thesis' implementation an refined version of this process is used:
\begin{enumerate*}
	\item at initialization time, each user task receives a service rate $s$ and a task variability $t$ value
	\item in the class initialization method, each user task samples an average processing time following an Erlang distribution (a special case of the gamma distribution) which takes as input parameters a shape $k$ and a scale $\theta$. The shape value $k$, as the name suggests, defines the curve shape that the Erlang distribution will follow. In this case both values $k$ and $\theta$ are dynamically evaluated at runtime as $k=s/t$ and $\theta = t$. This concept is depicted in \lstref{lst:user_task}
	\item the average processing time becomes a unique value of each user task object and is used by each policy to sample each user's service time, again from an Erlang sampled pool as depicted in \lstref{lst:user_service_rate} and we shall call this value $p_j$
\end{enumerate*}

\lstref{lst:user_service_rate} gives a glimpse of the inner logic of how policies work. It is however out of scope for this section to cover this aspect and it is provided ``as is''. For each user eligible to work the assigned token, its service rate is sampled following the Erlang distribution. This time, the Erlang distribution takes as parameters the unique average processing time $p_j$ of user task $j$ and a value worker variability, which is a unique property of each policy, which we shall call $w$.

In order to sample a service rate $p_{ij}$ following the Erlang distribution for each user $i$, shape $k$ is evaluated as $k=p_j/w$ and scale as $\theta = w$ as it can be seen in \lstref{lst:user_service_rate}



\begin{lstlisting}[caption=User service rate sampling following an Erlang distribution,label=lst:user_service_rate,language=Python]
    user_service_rate = [round(
            RANDOM_STATE.gamma(kbatchone_request_job.user_task.average_processing_time / self.worker_variability,
                               self.worker_variability),
            1) for _ in range(self.number_of_users)]
\end{lstlisting}

As previously mentioned, the Erlang distribution is a special case of the Gamma distribution where $k$ defines the shape of the curve. This distribution is better suited to model service rates since with an appropriate $k$ one can approximate a normal distribution without incurring in the aspect of having to manually reset negative values to one (thus loosing statistical generality). This is asserted by the formal definition of Erlang's support with $x \in [0,\infty)$.

\texttt{NumPy}'s implementation of its Erlang distribution is used\foot{https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.gamma.html}{06.01.2017}. \equref{eq:erlang_density} defines the probability density function of the Erlang's distribution with the alternative parametrization that uses $\mu$ instead of $\lambda$ as scale parameter, which is its reciprocal. This corresponds to the NumPy's implementation.

\begin{equation}
\label{eq:erlang_density}
	f(x;k,\mu) = \frac{x^{k-1} e^{-\frac{x}{\mu}}}{\mu^k (k-1)!} \text{ for } x,\mu \geq 0
\end{equation}

Each user task object has a claim token method, which takes tokens as input parameters and finally makes a call to its designed policy, passing the token. On this top level, without stepping into the single policies implementations, the logic is straightforward: start events generate tokens, user tasks that are direct children of start events claim the newly generated tokens, ask to the designated policies to work the token assigned to them and finally, after a service interval timeout which corresponds to the user's specific service interval, they release the token. The logic can be seen in \lstref{lst:user_task}.

\begin{lstlisting}[caption=User task initialization,label=lst:user_task,language=Python]
    class UserTask(object):
    def __init__(self, env, policy, name, knowledge, service_interval, task_variability):
        self.env = env
        self.policy = policy
        self.name = name
        self.knowledge = knowledge
        self.child = None
        self.average_processing_time = round(RANDOM_STATE.gamma(service_interval / task_variability, task_variability),
                                             1)

    def claim_token(self, token):
        policy_job = self.policy.request(self, token)
        service_time = yield policy_job.request_event
        yield self.env.timeout(service_time)
        self.policy.release(policy_job)
\end{lstlisting}

\subsubsection{Policy}

Different types of policies have been implemented following the foundations laid by Zeng and Zhao as outlined in \subsecref{subsec:workflow}. In their work the authors investigate five ``role-resolution'' policies used for optimal task to user assignment \cite[p. 7]{Zeng2005}. Following a brief description of the five aforementioned policies:

\begin{enumerate}
	\item A load balancing policy consists in assigning a task as soon as it arrives to a qualified worker with the shortest task queue at that moment. In this policy workers execute tasks assigned to them on a FIFO fashion. The authors call this policy the ``least loaded qualified person'' or LLQP.
	\item A policy that maintains a single queue being shared among all users is referred to the authors as ``shared queue'' or SQ policy.
	\item Another policy that maintains both a shared queue among all users and each user having an own queue and transfers tasks from the former to the latter is called ``K-Batch'' policy. Transfer of tasks from the shared queue to users is done using an optimal task assignment procedure as soon as the shared queue reaches a critical batch size $K$.
	\item The following policies takes the ``K-Batch'' policy but reduces the individual queue size of each user to one. This means that the optimization problem is still being solved as soon as the shared queue reaches the critical size $K$, however actual movement of tasks from the shared queue to the individual user queue happens only when user $i$ is not busy, \ie his individual queue is empty at simulation time $t$. This policy is called according to the authors as ``K-Batch-1''
	\item The last policy further simplifies the fourth by weakening the batch size constraint and reduces it to one. This means that the optimal task assignment procedure is executed immediately. This policy is referred by the authors as ``1-Batch-1''.
\end{enumerate}

All batching policies require the solution of an optimization problem. The authors define this problem as ``minimizing the maximum flowtime given the dynamic availability of the workers'' and call it ``minimizing sequential assignment (MSA)''\cite[p. 7]{Zeng2005}. The authors define the task flowtime as the elapsed simulation time between task generation and its completion \cite{Zeng2005,Baker1974}. Formally MSA is formulated as follows:

\begin{align}
    \min_z \quad z\\
    \text{subject to: } \notag \\
    \sum_{i \in W} x_{ij} &= 1 \quad \forall j \in T\\
    a_i + \sum_{j \in T} x_{ij} p_{ij} &\leq z \quad \forall i \in W\\
    x_{ij} \quad \text{or} \quad x_{ij}&=1 \quad \forall i \in W, \forall j \in T
\end{align}

All variables definition still hold without loss of generality as defined by the authors \cite[pp. 5-7]{Zeng2005}.

The class inheritance structure of the policies implementation with the corresponding fields and methods can be seen in \figref{fig:policies}.

\fig[\textwidth]{policies}{Policies class structure}{fig:policies}

The authors definition of the MSA problem is however a simplified version of the actual problem of ``minimizing the maximum task flowtime'' (MF) as defined by Baker \cite{Baker1974} with consideration of the dynamic arrival of tasks problem, defined by the authors as the DMF problem \cite{Zeng2005}. The DMF problem is formally defined by Zeng as follows:

\begin{align}
    \min_z \quad z\\
    \text{subject to: } \notag \\
    \sum_{i \in W} \sum_{k \in T} x_{ijk} &= 1 \quad \forall j \in T\\
    s_j &\geq r_j \quad \forall j \in T\\
    (x_{ijk} + x_{ij'(k+1)} - 1)(s_j + p_{ij}) &\leq s_{j'} \quad \forall i \in W, \forall k \in T, \forall j \in T, \forall j' \in T \label{eq:nonlinear_constraints_dmf}\\
    s_j + \sum_{i \in W} \sum_{k \in T} p_{ij} x_{ijk} - r_j &\leq z \quad \forall j \in T\\
    x_{ijk} = 0 \quad \text{or} \quad x_{ijk} = 1 &\quad \forall i \in W, \forall j \in T, \forall k \in T\\
    s_j &\geq 0
\end{align}

Again, all variables definition still hold without loss of generality as described by the authors \cite[p. 6]{Zeng2005}. As Zeng notes in his work, \equref{eq:nonlinear_constraints_dmf} contains nonlinear constraints but mentions that by adding auxiliary variables the aforementioned DMF formulation can be effectively converted into a mixed integer program and thus solve it \cite[p. 6]{Zeng2005}. On this note Zeng argues that the application of the DMF problem in practice poses some problems \cite{Zeng2005}. In this thesis however a conversion of the DMF formulation proposed by Zeng is formulated in order to adequately solve the optimization problem. The formal definition of such optimization problem is called EDMF (which stands for extended DMF) and is devised as follows:

\begin{align}
    \min_{z_{\text{max}}} \quad z_{\text{max}}\\
    \text{subject to: } \notag \\
    \sum_{i \in W} \sum_{k \in T} x_{ijk} &= 1 \quad \forall j \in T\\
    a_i + \sum_{j \in T} p_{ij} x_{ijk} &\leq z_{i*k} \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k=0\\
    z_{i*k-1} + \sum_{j \in T} p_{ij} x_{ijk} &\leq z_{i*k} \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k>0\\
    z_{i*k}+ \sum_{j \in T} w_j x_{ijk} &\leq z_{\text{max}} \quad \forall i \in W, \forall k \in T\\
    \sum_{j \in T} x_{ijk} &\leq 1 \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k=0\\
    \sum_{j \in T} x_{ijk} &\leq \sum_{j \in T} x_{ijk-1} \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k>0\\
    z_{i*k} &\geq 0 \quad \forall i \in W, \forall k \in T
\end{align}

This formulation clearly gets rid of the nonlinear constraints while still accounting for dynamical arrival of tasks, making thus the DMF problem as defined by Zeng effectively solvable.

When considering the minimization of the maximum flowtime of a task inside a process, the EDMF formulation can be further simplified by adopting some assumptions about the order and sequence of tasks. Based on how the batching policies are implemented, the policy job objects to be worked by users are implicitly stored in a sorted fashion. This means that the $z$ helper variables defined for EDMF are not strictly necessary and thus can be compressed by \equref{eq:simplified_z_with_k}:

\begin{equation}
\label{eq:simplified_z_with_k}
	a_i + \sum_{t=1}^k \sum_j (p_{ij} + w_j I(t=k))x_{ijt}
\end{equation}

The whole concept consists in the introduction of an identity variable $I$ which is true only if task $j$ is currently being assigned as the $k$th task to user $i$, meaning that for this specific case also the waiting time for task $j$ has to be accounted for. For all other cases, \ie $j<k$ the identity variable $I$ will not hold thus effectively zeroing the $w_j$ variable.

\figref{fig:edmf_task_assignment} depicts the potential scenario where three tasks are assigned to a specific user $i$ following a sequence where task 2, 3 and $j$ are assigned respectively as first, second and third tasks (thus respecting the $k$ notation outlined above them).

In order to calculate $z_{ijk}$, one has to consider also when user $i$ will actually be available to process his first task. This is depicted by the variable $a_i$, which summed together with the respective service times of user $i$ for task $j$ gives the complete work time user $i$ will require to process all tasks assigned to him.

\fig[\textwidth]{dmf_problem}{EDMF Task Assignment}{fig:edmf_task_assignment}

Without further ado, the simplified formulation of the extended DMF variant (called SDMF, which stands for simplified DMF) is the following:

\begin{align}
    \min_{z_{\text{max}}} \quad z_{\text{max}}\\
    \text{subject to: } \notag \\
    \sum_{i \in W} \sum_{k \in T} x_{ijk} &= 1 \quad \forall j \in T\\
    a_i + \sum_{t=1}^k \sum_j (p_{ij} + w_j I(t=k))x_{ijt} &\leq z_{\text{max}}\\
    \sum_{j \in T} x_{ijk} &\leq 1 \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k=0\\
    \sum_{j \in T} x_{ijk} &\leq \sum_{j \in T} x_{ijk-1} \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k>0
\end{align}

By comparing both formulation it is clear that SDMF manages to simplify the mathematical formulation and relaxing the required amount of constraints while still attaining the same level of effectiveness. Please note, however, that this simplification is only possible because of the nature of the implementation.

Based on this approach and by further exploiting the implicit order implementation of task arrival in the global queues for both batching policies, it is possible to argue that the $k$ sequence indexing can be relaxed as well, thus even further simplifying the mathematical formulation and respectively the optimization problem size and computation costs.

The formulation of the DMF problem by relaxing both the $z$ variables and $k$ indexes, it is possible to formulate the same DMF problem as follows:

\begin{align}
    \min_{z_{\text{max}}} \quad z_{\text{max}}\\
    \text{subject to: } \notag \\
    \sum_{i \in W} x_{ij} &= 1 \quad \forall j \in T\\
    a_i + \sum_{k=1}^j (p_{ik} + w_k I(k=j))x_{ik} &\leq z_{\text{max}}
\end{align}

This formulation is colloquially called the ESDMF method (extremely simplified DMF).

\todo{add explanation that by exploiting how the arrival order is implemented one can solve the same DMF problem without increasing the computability costs.}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[htb]
\centering
\begin{tabular}{@{}lc@{}}
\toprule
Solver & Computation Costs \\ \midrule
MSA    & $O(mn)$           \\
EDMF   & $O(mn^2)$         \\
SDMF   & $O(mn^2)$         \\
ESDMF  & $O(mn)$           \\ \bottomrule
\end{tabular}
\caption{Comparison of computational costs for different solvers}
\label{tab:big_oh_solvers}
\end{table}

Sum of service times minimization with DMF as upper bound:

\begin{align}
    \min_z \quad \sum_{i \in W} \sum_{k \in T} z_{ik}\\
    \text{subject to: } \notag \\
    \sum_{i \in W} \sum_{k \in T} x_{ijk} &= 1 \quad \forall j \in T\\
    a_i + \sum_{j \in T} p_{ij} x_{ijk} - M(1 - \sum_{j \in T} x_{ijk}) &\leq z_{i*k} \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k=0\\
    z_{i*k-1} + \sum_{j \in T} p_{ij} x_{ijk} - M(1 - \sum_{j \in T} x_{ijk}) &\leq z_{i*k} \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k>0\\
    z_{i*k}+ \sum_{j \in T} w_j x_{ijk} &\leq z_{\text{max}} + \epsilon \quad \forall i \in W, \forall k \in T\\
    \sum_{j \in T} x_{ijk} &\leq 1 \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k=0\\
    \sum_{j \in T} x_{ijk} &\leq \sum_{j \in T} x_{ijk-1} \quad \forall i \in W, \forall k \in T \quad \text{for} \quad k>0\\
    z_{i*k} &\geq 0 \quad \forall i \in W, \forall k \in T\\
    M &= \max_a a_i + \max_p \sum_{i \in W} \sum_{j \in T} p_{ij}\\
    \epsilon &= \num{1e-4}
\end{align}

\section{Reinforcement Learning Analysis Environment}

In this section the reinforcement learning approach used to solve the different role resolution problems is depicted. Initially a foundation basis in the required knowledge is depicted and afterwards the description of the analysis environment implementation is presented.

\subsection{Reinforcement Learning}

Reinforcement learning is a novel approach originated as a branch from the broader field of machine learning. It is an automated approach to understanding and automating learning and decision-making \cite[p. 15]{Sutton1998}. It distinguishes itself from other approaches by its novel focus on learning thanks to an agent which directly interacts with its environment, without the necessity of relying on training sets \cite[p. 15]{Sutton1998}.

The formal framework used by reinforcement learning defines the interaction between the so called learning agent and its environment by means of states, actions and rewards \cite[p. 15]{Sutton1998}.

Key concepts in the field of reinforcement learning are those of values and value functions which helps distinguish reinforcement learning methods from evolutionary methods which have to undergo scalar evaluations of entire policies \cite[p. 15]{Sutton1998}.

\subsection{Finite Markov Decision Processes}

Reinforcement learning approaches learn by interacting with the environment in order to achieve a goal. The agent interacting with the environment does this in a sequence of discrete time steps, it performs actions (choices made by the agent), reaches then states (basis for making decisions) and eventually receives rewards (basis for evaluating the choices) \cite[p. 73]{Sutton1998}. Moreover, a policy is a stochastic rule that the agent relies upon to choose actions as a function of states \cite[p. 73]{Sutton1998}. Ultimately, the sole goal of the agent is to maximize the reward that it receives over time \cite[p. 73]{Sutton1998}.

Returns are modeled as functions of future rewards that an agents must maximize \cite[p. 73]{Sutton1998}. There exist two types of return functions which depend on the nature of the tasks and a discounting preference:
\begin{enumerate*}
	\item for episodic tasks a non discontinued approach is preferred while
	\item for continuous tasks, however, a discounted approach is better suited \cite[p. 73]{Sutton1998}.
\end{enumerate*}

\equref{eq:expected_return} defines the sum of the rewards received over time step $t$:

\begin{equation}
\label{eq:expected_return}
	G_t  \doteq R_{t+1} + R_{t+2} + R_{t+3} + \cdots R_{T}
\end{equation}

If we account for discounting, \equref{eq:expected_return} has to be slightly adapted by introducing a discounting factor $\gamma$ and can be found in \equref{eq:expected_discounted_return}:

\begin{equation}
\label{eq:expected_discounted_return}
	G_t  \doteq R_{t+1} + R_{t+2} + R_{t+3} + \cdots = \sum_{k=0}^\infty \gamma^k R_{t+k+1}
\end{equation}

where $0 \leq \gamma \leq 1$.

An environment  with which one agent interacts, can satisfy a Markov property if the information contained at present effectively summarizes the past without affecting the capability of effectively predicting the future \cite[p. 73]{Sutton1998}. If the Markov property is satisfied, then this environment is called a Markov decision process (MDP) \cite[p. 73]{Sutton1998}.

Last but not least, value functions are used to assign each state or state-action pair an expected return based on the policy used by the agent \cite[p. 74]{Sutton1998}. Optimal value functions assign the highest achievable return by any policy to a state or state-action pair  and such policies, whose values are optimal, are called optimal policies \cite[p. 74]{Sutton1998}.

Optimal state-value functions $v_*$ are formally defined as follows:

\begin{equation}
	v_* (s) \doteq \max_\pi v_\pi (s)
\end{equation}

whereas optimal action-value functions $q_*$ are formally defined as follows:

\begin{equation}
	q_* (s,a) \doteq \max_\pi q_\pi (s,a)
\end{equation}

\subsection{Dynamic Programming}
\label{subsec:dp}

Dynamic programming (DP) is a set of ideas and algorithms that can be used to solve MDPs \cite[p. 95]{Sutton1998}. There are two approaches in dynamic programming for solving MDPs:
\begin{enumerate*}
	\item policy evaluations is the iterative computation of value functions of a given policy and
	\item policy improvement is the idea of computing an improved policy under the conditions of its given value functions \cite[p. 95]{Sutton1998}.
\end{enumerate*}

By combining these two approaches we obtain the two most notable DP methods, \ie policy and value iteration \cite[p. 95]{Sutton1998}.

One very interesting property of DP methods is the concept of bootstrapping: updating estimates of values of states by approximating the values of future states \cite[p. 96]{Sutton1998}.

\subsection{Monte Carlo Methods}
\label{subsec:mc}

Monte Carlo (MC) methods use experience in form of sample episodes in order to learn value functions and optimal policies \cite[p. 123]{Sutton1998}. This approach yields different advantages over the DP methods seen in \secref{subsec:dp}:
\begin{enumerate*}
	\item they do not need a model of the environment's dynamics as they learn the optimal solutions by merely interacting with the environment,
	\item since the learn from sample episodes, they are very well suited for simulation environments,
	\item it is efficient and surprisingly easy to use MC methods to focus on smaller regions or subsets of a problem and
	\item MC methods are more robust when it comes to violations of the Markov property since they do not bootstrap for updating their values \cite[p. 123]{Sutton1998}.
\end{enumerate*}

One of the drawbacks that MC methods bring along is the concept of maintaining sufficient exploration: by always acting greedily, alternative states will never yield their returns thus potentially never learning that they might prove to be better \cite[p. 123]{Sutton1998}.

A MC simplified method can be formally defined as follows:

\begin{equation}
	V(S_t) \leftarrow V(S_t) + \alpha [G_t - V(S_t)]
\end{equation}

where $G_t$ is the discounted return function defined by \equref{eq:expected_discounted_return} and $\alpha$ is a constant step-size parameter \cite[p. 127]{Sutton1998}. MC methods must wait until the end of one episode in order to evaluate the incremental value of $V(S_t)$ since only at that point in time $G_t$ is known \cite[p. 128]{Sutton1998}.

\subsection{Temporal-Difference Learning}

Temporal-difference (TD) are yet another set of learning methods for reinforcement learning. Compared to the MC methods explained in \subsecref{subsec:mc}, TD methods do not need to wait all the way up to the end of an episode to actually learn, they only must wait until the next step, \ie they can bootstrap \cite[p. 128]{Sutton1998}. When they reach time step $t+1$, they observe a reward $R_{t+1}$ which then use to estimate $V(S_{t+1})$ \cite[p. 128]{Sutton1998}. The simplest TD method, which is called $TD(0)$, is defined as follows:

\begin{equation}
	V(S_t) \leftarrow V(S_t) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(S_t)]
\end{equation}

TD methods, same as MC methods, are not  excluded from the sufficient exploration methods \cite[p. 147]{Sutton1998}. TD methods deal with this complication in two different ways:
\begin{enumerate*}
	\item on policy by using an algorithm called Sarsa and
	\item off policy by using an algorithm called Q-learning \cite[p. 128]{Sutton1998}.
\end{enumerate*}

\subsection{On-policy Prediction with Approximation}

Up until now, the different methods presented are not suited for arbitrarily large state spaces. However, there exist solution to tackle such large state spaces: approximate solution methods. Under the assumption that one must always account for finite and limited computational resources, it is not feasible to find an optimal policy or value function, instead we have to settle for a good approximation of the solution \cite[p. 189]{Sutton1998}.

An essential characteristic for reinforcement learning algorithms venturing in the area of approximation is being able of generalization, \ie using experience from a limited subset of the state space to effectively generalize and produce a valid approximation of a much larger subset  \cite[p. 189]{Sutton1998}. Reinforcement learning methods are capable of achieving this by relying on supervised-learning function approximation which essentially use backups as training example \cite[p. 222]{Sutton1998}. Specifically, one very efficient set of methods are those using parametrized function approximation, \ie the policy is parametrized by a weight vector $\theta$.

The parametrized functional form with weight vector $\theta$ can be used to write $\hat{v}(s,\theta) \approx v_\pi (s)$, which is the approximated value of state $s$ given weight vector $\theta$ \cite[p. 191]{Sutton1998}.

It is then clear that the weight vector $\theta$ has to be chosen wisely: this can be done by using variations of stochastic gradient descent (SGD) methods \cite[p. 223]{Sutton1998}. SGD methods adjust the weight vector after each step by a tiny amount following the direction that would reduce the error the most:

\begin{align}
	\theta_{t+1} &\doteq \theta_t - \frac{1}{2} \alpha \nabla [v_\pi (S_t) - \hat{v} (S_t,\theta_t)]^2\\
	&= \theta_t + \alpha  [v_\pi (S_t) - \hat{v} (S_t,\theta_t)] \nabla \hat{v} (S_t,\theta_t)
\end{align}

where $\alpha$ is a positive step size parameter and $\nabla f(\theta)$ is the vector of partial derivatives with respect to $\theta$:

\begin{equation}
	\nabla f(\theta) \doteq \left( \frac{\partial f(\theta)}{\partial \theta_1}, \frac{\partial f(\theta)}{\partial \theta_2}, \ldots, \frac{\partial f(\theta)}{\partial \theta_n} \right)^\top
\end{equation}

\ncite{add book citation}

A very special case (and also very simple) is linear methods for function approximation, where the approximate function $\hat{v} (\cdot ,\theta)$ is a linear function of the weight vector $\theta$ \cite[p. 198]{Sutton1998}. This means that for each state $s$ there is a corresponding vector of features $\phi (s) \doteq \left( \phi_1 (s), \phi_2 (s), \ldots, \phi_n (s) \right)^\top$ which has the same number of components as $\theta$ \cite[p. 198]{Sutton1998}. With this definition in mind, we can now formally define the state-value function approximation as the inner product between $\theta$ and $\phi (s)$:

\begin{equation}
	\hat{v} (s,\theta) \doteq \theta^\top \phi (s) \doteq \sum_{i=1}^n \theta_i \phi_i (s)
\end{equation}

This simplified case of linear function approximation for state-value functions finally brings us to how we can use the SGD:

\begin{equation}
\label{eq:sgd_linear}
	\nabla \hat{v} (s,\theta) = \phi (s)
\end{equation}

\equref{eq:sgd_linear} tells us that for the simple linear case the SGD is nothing more than the corresponding features value  \cite[p. 199]{Sutton1998}.

\subsection{On-policy Control with Approximation}

Moving towards control with value function approximation, we now focus on the approximation of the action-value function $\hat{q} (s,a,\theta) \approx q_* (s,a)$ \cite[p. 229]{Sutton1998}.

For the special case of the so called one-step Sarsa method, its gradient-descent update for the action-value function is defined as follows:

\begin{equation}
	\theta_{t+1} \doteq \theta_t + \alpha [ R_{t+1} + \gamma \hat{q} (S_{t+1}, A_{t+1}, \theta_t) - \hat{q} (S_t, A_t, \theta_t) ] \nabla \hat{q} (S_t, A_t, \theta_t)
\end{equation}

and this method has also very good convergence properties towards optimality \cite[p. 230]{Sutton1998}.

\subsection{Policy Gradient Methods}

Up until now all methods were based on the concept of learning values of actions and subsequently choosing the correct actions based on estimates, however, we now move our focus towards methods that actually learn a parametrized policy without needing value functions at all\footnote{Actor-critic methods are an exception, where a learned value function is used in combination with policy gradient as a baseline in order to lower variance.}  \cite[p. 265]{Sutton1998}. Parametrized policies work with probabilities that a specific action $a$ will be chosen at time $t$ if the agent finds itself in state $s$ at time $t$ with a weight vector $\theta$ \cite[p. 265]{Sutton1998}. For policy gradient methods it is crucial to learn the weight vector based on a performance measure $\eta(\theta)$ by trying to maximize and thus approximating the gradient ascent of $\eta$ as follows:

\begin{equation}
	\theta_{t+1} \doteq \theta_t + \alpha \widehat{\nabla \eta (\theta_t)}
\end{equation}

where $ \widehat{\nabla \eta (\theta_t)}$ is nothing else than a stochastic estimate that approximates the gradient of $\eta(\theta)$ \cite[p. 265]{Sutton1998}.

For discrete action spaces, a suitable solution consists in forming parametrized numerical preferences $h(s,a,\theta) \in \mathbb{R}$ \cite[p. 266]{Sutton1998}. This means that the best actions is given the highest probability according to a softmax distribution:

\begin{equation}
	\pi(a|s,\theta) \doteq \frac{e^{h(s,a,\theta)}}{\sum_b e^{h(s,b,\theta)}}
\end{equation}

where $e \approx 2.71828$ \cite[p. 266]{Sutton1998}.

Moreover, the preferences can be, as previously mentioned:

\begin{equation}
	h(s,a,\theta) \doteq \theta^\top \phi (s,a)
\end{equation}

\ie simply linear in features \cite[p. 266]{Sutton1998}.

\section{Hypothesis}
\section{Data}

\chapter{Empirical Analysis}
\label{ch:empirical_analysis}



\section{Results}
\section{Discussion}
\section{Research Contribution}

\chapter{Conclusion}
\label{ch:conclusion}

\section{Summary}
\section{Resulting Conclusions}
\section{Outlook}

\appendix

\chapter{test}
\chapter{test2}

\backmatter


\bibliographystyle{alpha}
\bibliography{sources}

\end{document}
