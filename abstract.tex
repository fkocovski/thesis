\begin{abstract}

Efficiently assigning human resources in \glspl{wfms} is a vital aspect when implementing them in corporate environments \citep{Cheng2000,Mentzas2001}. 

The traditional approach formulates a deterministic planning model using \gls{milp} and solves role resolution to optimality which has lead to promising results \citep{Zeng2005}. However the deterministic approach is a limiting factor when approaching role resolution with \gls{milp} \citep{Zeng2005}.

This thesis expands on existing work in the field of role resolution in \glspl{wfms} by extending the already researched \gls{milp} methodologies and proposes a novel approach by introducing \gls{rl} based approaches \citep{Sutton2017}. These modern approaches help overcoming the deterministic limitations of \gls{milp} by approaching role resolution in a stochastic fashion \citep{Sutton2017}.

In this project a subset of \gls{bpmn} elements have been implemented in a discrete event simulation environment for evaluation, in which existing policies for role resolution in \glspl{wfms}, such as \gls{sq}, \gls{llqp}, K-Batch and 1-Batch-1 are tested.

Both the extended \gls{milp} as well as the \gls{rl} based methods outperform the traditional approaches up to a $1.3$-fold speedup. Even though promising results have been obtained, precautions have to be taken when interpreting the results: on one hand the \gls{milp} based optimizations obtained are coupled with higher computational complexity which raise business trade-offs. On the other hand, \gls{rl} based optimizations overcome the computational complexity problem of \gls{milp} based methods but require lengthy training sessions in order to assert optimal convergence.

\gls{rl} based methods lay the foundations for extensions by using alternative methods such as \gls{irl} \citep{Ng2000} and \gls{al}. \gls{al} promises to overcome the limitations of \gls{rl} methods by eliminating the requirement of internal reward functions and learn optimal behaviors by ``observing'' experts executing tasks \citep{Abbeel2004}. Future work could reconcile traditional \gls{milp} and \gls{al} based methods by using the former as the ``expert'' agent performing role resolution and the latter observing its behavior in order to learn from it.

\end{abstract}